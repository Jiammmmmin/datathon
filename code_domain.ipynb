{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 122'}\n"
     ]
    }
   ],
   "source": [
    "train_val_data = pd.read_csv(\"./Train_and_Validate_EEG.csv\")\n",
    "headers_before = train_val_data.columns.to_list()\n",
    "\n",
    "train_val_data = train_val_data.dropna(axis=1, how='all')\n",
    "headers_after = train_val_data.columns.to_list()\n",
    "\n",
    "print(set(headers_before) - set(headers_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>eeg.date</th>\n",
       "      <th>education</th>\n",
       "      <th>IQ</th>\n",
       "      <th>main.disorder</th>\n",
       "      <th>specific.disorder</th>\n",
       "      <th>AB.A.delta.a.FP1</th>\n",
       "      <th>AB.A.delta.b.FP2</th>\n",
       "      <th>...</th>\n",
       "      <th>COH.F.gamma.o.Pz.p.P4</th>\n",
       "      <th>COH.F.gamma.o.Pz.q.T6</th>\n",
       "      <th>COH.F.gamma.o.Pz.r.O1</th>\n",
       "      <th>COH.F.gamma.o.Pz.s.O2</th>\n",
       "      <th>COH.F.gamma.p.P4.q.T6</th>\n",
       "      <th>COH.F.gamma.p.P4.r.O1</th>\n",
       "      <th>COH.F.gamma.p.P4.s.O2</th>\n",
       "      <th>COH.F.gamma.q.T6.r.O1</th>\n",
       "      <th>COH.F.gamma.q.T6.s.O2</th>\n",
       "      <th>COH.F.gamma.r.O1.s.O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>852.000000</td>\n",
       "      <td>852</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852</td>\n",
       "      <td>839.000000</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>852</td>\n",
       "      <td>852</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "      <td>852.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013.1.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mood disorder</td>\n",
       "      <td>Depressive disorder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>555</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>250</td>\n",
       "      <td>191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>471.409624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.548744</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.430274</td>\n",
       "      <td>101.396429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.330829</td>\n",
       "      <td>21.312239</td>\n",
       "      <td>...</td>\n",
       "      <td>75.842007</td>\n",
       "      <td>54.983694</td>\n",
       "      <td>56.947577</td>\n",
       "      <td>60.557073</td>\n",
       "      <td>69.813658</td>\n",
       "      <td>47.847909</td>\n",
       "      <td>66.709123</td>\n",
       "      <td>39.249992</td>\n",
       "      <td>66.130099</td>\n",
       "      <td>56.903789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>274.989580</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.738390</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.564135</td>\n",
       "      <td>17.085601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.443365</td>\n",
       "      <td>12.417935</td>\n",
       "      <td>...</td>\n",
       "      <td>15.782265</td>\n",
       "      <td>19.511627</td>\n",
       "      <td>18.203185</td>\n",
       "      <td>17.744601</td>\n",
       "      <td>17.748133</td>\n",
       "      <td>19.598164</td>\n",
       "      <td>16.843523</td>\n",
       "      <td>20.843316</td>\n",
       "      <td>17.917650</td>\n",
       "      <td>19.604724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.272260</td>\n",
       "      <td>3.244199</td>\n",
       "      <td>...</td>\n",
       "      <td>12.611954</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>6.926792</td>\n",
       "      <td>2.214436</td>\n",
       "      <td>2.421748</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>1.032207</td>\n",
       "      <td>1.228502</td>\n",
       "      <td>1.643951</td>\n",
       "      <td>4.340159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>230.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.752500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.772447</td>\n",
       "      <td>13.003745</td>\n",
       "      <td>...</td>\n",
       "      <td>68.067727</td>\n",
       "      <td>41.659341</td>\n",
       "      <td>44.045125</td>\n",
       "      <td>48.632636</td>\n",
       "      <td>59.196016</td>\n",
       "      <td>32.655800</td>\n",
       "      <td>56.154806</td>\n",
       "      <td>22.041208</td>\n",
       "      <td>54.987797</td>\n",
       "      <td>43.834643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>472.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.010000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.137013</td>\n",
       "      <td>17.930642</td>\n",
       "      <td>...</td>\n",
       "      <td>78.624002</td>\n",
       "      <td>55.105995</td>\n",
       "      <td>56.404859</td>\n",
       "      <td>61.108607</td>\n",
       "      <td>72.173855</td>\n",
       "      <td>45.701515</td>\n",
       "      <td>67.985444</td>\n",
       "      <td>36.305708</td>\n",
       "      <td>68.081346</td>\n",
       "      <td>57.180194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>710.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.535000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.816483</td>\n",
       "      <td>25.992534</td>\n",
       "      <td>...</td>\n",
       "      <td>87.165614</td>\n",
       "      <td>69.635095</td>\n",
       "      <td>70.607069</td>\n",
       "      <td>73.696268</td>\n",
       "      <td>83.026180</td>\n",
       "      <td>61.997235</td>\n",
       "      <td>78.736665</td>\n",
       "      <td>54.004810</td>\n",
       "      <td>78.978633</td>\n",
       "      <td>71.045497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.880000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.826192</td>\n",
       "      <td>101.515687</td>\n",
       "      <td>...</td>\n",
       "      <td>99.603886</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.307895</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.581629</td>\n",
       "      <td>98.720067</td>\n",
       "      <td>99.650154</td>\n",
       "      <td>98.413320</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.287092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 1148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  sex         age   eeg.date   education          IQ  \\\n",
       "count   852.000000  852  852.000000        852  839.000000  840.000000   \n",
       "unique         NaN    2         NaN        666         NaN         NaN   \n",
       "top            NaN    M         NaN  2013.1.17         NaN         NaN   \n",
       "freq           NaN  555         NaN          5         NaN         NaN   \n",
       "mean    471.409624  NaN   30.548744        NaN   13.430274  101.396429   \n",
       "std     274.989580  NaN   11.738390        NaN    2.564135   17.085601   \n",
       "min       1.000000  NaN   18.000000        NaN    0.000000   49.000000   \n",
       "25%     230.750000  NaN   21.752500        NaN   12.000000   91.000000   \n",
       "50%     472.500000  NaN   26.010000        NaN   13.000000  102.000000   \n",
       "75%     710.250000  NaN   35.535000        NaN   16.000000  114.000000   \n",
       "max     945.000000  NaN   71.880000        NaN   20.000000  145.000000   \n",
       "\n",
       "        main.disorder    specific.disorder  AB.A.delta.a.FP1  \\\n",
       "count             852                  852        852.000000   \n",
       "unique              7                   12               NaN   \n",
       "top     Mood disorder  Depressive disorder               NaN   \n",
       "freq              250                  191               NaN   \n",
       "mean              NaN                  NaN         20.330829   \n",
       "std               NaN                  NaN         11.443365   \n",
       "min               NaN                  NaN          3.272260   \n",
       "25%               NaN                  NaN         12.772447   \n",
       "50%               NaN                  NaN         17.137013   \n",
       "75%               NaN                  NaN         24.816483   \n",
       "max               NaN                  NaN         92.826192   \n",
       "\n",
       "        AB.A.delta.b.FP2  ...  COH.F.gamma.o.Pz.p.P4  COH.F.gamma.o.Pz.q.T6  \\\n",
       "count         852.000000  ...             852.000000             852.000000   \n",
       "unique               NaN  ...                    NaN                    NaN   \n",
       "top                  NaN  ...                    NaN                    NaN   \n",
       "freq                 NaN  ...                    NaN                    NaN   \n",
       "mean           21.312239  ...              75.842007              54.983694   \n",
       "std            12.417935  ...              15.782265              19.511627   \n",
       "min             3.244199  ...              12.611954               0.519048   \n",
       "25%            13.003745  ...              68.067727              41.659341   \n",
       "50%            17.930642  ...              78.624002              55.105995   \n",
       "75%            25.992534  ...              87.165614              69.635095   \n",
       "max           101.515687  ...              99.603886             100.000000   \n",
       "\n",
       "        COH.F.gamma.o.Pz.r.O1  COH.F.gamma.o.Pz.s.O2  COH.F.gamma.p.P4.q.T6  \\\n",
       "count              852.000000             852.000000             852.000000   \n",
       "unique                    NaN                    NaN                    NaN   \n",
       "top                       NaN                    NaN                    NaN   \n",
       "freq                      NaN                    NaN                    NaN   \n",
       "mean                56.947577              60.557073              69.813658   \n",
       "std                 18.203185              17.744601              17.748133   \n",
       "min                  6.926792               2.214436               2.421748   \n",
       "25%                 44.045125              48.632636              59.196016   \n",
       "50%                 56.404859              61.108607              72.173855   \n",
       "75%                 70.607069              73.696268              83.026180   \n",
       "max                 99.307895             100.000000              99.581629   \n",
       "\n",
       "        COH.F.gamma.p.P4.r.O1  COH.F.gamma.p.P4.s.O2  COH.F.gamma.q.T6.r.O1  \\\n",
       "count              852.000000             852.000000             852.000000   \n",
       "unique                    NaN                    NaN                    NaN   \n",
       "top                       NaN                    NaN                    NaN   \n",
       "freq                      NaN                    NaN                    NaN   \n",
       "mean                47.847909              66.709123              39.249992   \n",
       "std                 19.598164              16.843523              20.843316   \n",
       "min                  0.036664               1.032207               1.228502   \n",
       "25%                 32.655800              56.154806              22.041208   \n",
       "50%                 45.701515              67.985444              36.305708   \n",
       "75%                 61.997235              78.736665              54.004810   \n",
       "max                 98.720067              99.650154              98.413320   \n",
       "\n",
       "        COH.F.gamma.q.T6.s.O2  COH.F.gamma.r.O1.s.O2  \n",
       "count              852.000000             852.000000  \n",
       "unique                    NaN                    NaN  \n",
       "top                       NaN                    NaN  \n",
       "freq                      NaN                    NaN  \n",
       "mean                66.130099              56.903789  \n",
       "std                 17.917650              19.604724  \n",
       "min                  1.643951               4.340159  \n",
       "25%                 54.987797              43.834643  \n",
       "50%                 68.081346              57.180194  \n",
       "75%                 78.978633              71.045497  \n",
       "max                100.000000              99.287092  \n",
       "\n",
       "[11 rows x 1148 columns]"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(train_val_data.info())\n",
    "train_val_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['education', 'IQ'], dtype='object')"
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_counts = train_val_data.describe(include='all').iloc[0]\n",
    "idx_hasna = train_val_counts[train_val_counts.apply(lambda x: int(x) != 852)].index\n",
    "\n",
    "idx_hasna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[769, 707, 3, 199, 73, 587, 402, 537, 157, 226, 738, 803, 162, 227, 487, 804, 679, 362, 108, 242, 823, 250, 379, 316]\n"
     ]
    }
   ],
   "source": [
    "rows_with_na_education = train_val_data[train_val_data['education'].isna()]\n",
    "rows_with_na_iq = train_val_data[train_val_data['IQ'].isna()]\n",
    "\n",
    "na_index = list(set(rows_with_na_education.index.to_list()).union(set(rows_with_na_iq.index.to_list())))\n",
    "print(na_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>eeg.date</th>\n",
       "      <th>education</th>\n",
       "      <th>IQ</th>\n",
       "      <th>main.disorder</th>\n",
       "      <th>specific.disorder</th>\n",
       "      <th>AB.A.delta.a.FP1</th>\n",
       "      <th>AB.A.delta.b.FP2</th>\n",
       "      <th>...</th>\n",
       "      <th>COH.F.gamma.o.Pz.p.P4</th>\n",
       "      <th>COH.F.gamma.o.Pz.q.T6</th>\n",
       "      <th>COH.F.gamma.o.Pz.r.O1</th>\n",
       "      <th>COH.F.gamma.o.Pz.s.O2</th>\n",
       "      <th>COH.F.gamma.p.P4.q.T6</th>\n",
       "      <th>COH.F.gamma.p.P4.r.O1</th>\n",
       "      <th>COH.F.gamma.p.P4.s.O2</th>\n",
       "      <th>COH.F.gamma.q.T6.r.O1</th>\n",
       "      <th>COH.F.gamma.q.T6.s.O2</th>\n",
       "      <th>COH.F.gamma.r.O1.s.O2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>828.000000</td>\n",
       "      <td>828</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828</td>\n",
       "      <td>828</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "      <td>828.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013.1.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mood disorder</td>\n",
       "      <td>Depressive disorder</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>246</td>\n",
       "      <td>188</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>471.657005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.598152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.435990</td>\n",
       "      <td>101.375604</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.367918</td>\n",
       "      <td>21.375176</td>\n",
       "      <td>...</td>\n",
       "      <td>75.972853</td>\n",
       "      <td>55.115058</td>\n",
       "      <td>56.993255</td>\n",
       "      <td>60.662168</td>\n",
       "      <td>70.025798</td>\n",
       "      <td>47.940380</td>\n",
       "      <td>66.904196</td>\n",
       "      <td>39.407460</td>\n",
       "      <td>66.289039</td>\n",
       "      <td>57.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>274.566015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.747058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.570789</td>\n",
       "      <td>17.143543</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.537351</td>\n",
       "      <td>12.540608</td>\n",
       "      <td>...</td>\n",
       "      <td>15.757074</td>\n",
       "      <td>19.580062</td>\n",
       "      <td>18.261624</td>\n",
       "      <td>17.843551</td>\n",
       "      <td>17.700283</td>\n",
       "      <td>19.607299</td>\n",
       "      <td>16.808581</td>\n",
       "      <td>20.941359</td>\n",
       "      <td>17.922616</td>\n",
       "      <td>19.733269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.272260</td>\n",
       "      <td>3.244199</td>\n",
       "      <td>...</td>\n",
       "      <td>12.611954</td>\n",
       "      <td>0.519048</td>\n",
       "      <td>6.926792</td>\n",
       "      <td>2.214436</td>\n",
       "      <td>2.421748</td>\n",
       "      <td>0.036664</td>\n",
       "      <td>1.032207</td>\n",
       "      <td>1.228502</td>\n",
       "      <td>1.643951</td>\n",
       "      <td>4.340159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>230.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.835000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>91.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.788493</td>\n",
       "      <td>13.003745</td>\n",
       "      <td>...</td>\n",
       "      <td>68.803249</td>\n",
       "      <td>41.659341</td>\n",
       "      <td>43.927332</td>\n",
       "      <td>48.780868</td>\n",
       "      <td>59.579014</td>\n",
       "      <td>32.655800</td>\n",
       "      <td>56.479459</td>\n",
       "      <td>22.006032</td>\n",
       "      <td>55.242184</td>\n",
       "      <td>43.834643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>474.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.085000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.107741</td>\n",
       "      <td>17.877864</td>\n",
       "      <td>...</td>\n",
       "      <td>78.762700</td>\n",
       "      <td>55.368414</td>\n",
       "      <td>56.694682</td>\n",
       "      <td>61.243097</td>\n",
       "      <td>72.473002</td>\n",
       "      <td>45.877296</td>\n",
       "      <td>68.250270</td>\n",
       "      <td>36.627840</td>\n",
       "      <td>68.418304</td>\n",
       "      <td>57.581482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>709.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.587500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.816483</td>\n",
       "      <td>26.193861</td>\n",
       "      <td>...</td>\n",
       "      <td>87.191115</td>\n",
       "      <td>69.666736</td>\n",
       "      <td>70.689085</td>\n",
       "      <td>73.797222</td>\n",
       "      <td>83.410553</td>\n",
       "      <td>62.154134</td>\n",
       "      <td>79.079525</td>\n",
       "      <td>54.169384</td>\n",
       "      <td>79.172126</td>\n",
       "      <td>71.643516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>945.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.880000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>92.826192</td>\n",
       "      <td>101.515687</td>\n",
       "      <td>...</td>\n",
       "      <td>99.603886</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.307895</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.581629</td>\n",
       "      <td>98.720067</td>\n",
       "      <td>99.650154</td>\n",
       "      <td>98.413320</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>99.287092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 1148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  sex         age   eeg.date   education          IQ  \\\n",
       "count   828.000000  828  828.000000        828  828.000000  828.000000   \n",
       "unique         NaN    2         NaN        651         NaN         NaN   \n",
       "top            NaN    M         NaN  2013.1.17         NaN         NaN   \n",
       "freq           NaN  539         NaN          5         NaN         NaN   \n",
       "mean    471.657005  NaN   30.598152        NaN   13.435990  101.375604   \n",
       "std     274.566015  NaN   11.747058        NaN    2.570789   17.143543   \n",
       "min       1.000000  NaN   18.000000        NaN    0.000000   49.000000   \n",
       "25%     230.750000  NaN   21.835000        NaN   12.000000   91.000000   \n",
       "50%     474.500000  NaN   26.085000        NaN   13.000000  102.000000   \n",
       "75%     709.250000  NaN   35.587500        NaN   16.000000  114.000000   \n",
       "max     945.000000  NaN   71.880000        NaN   20.000000  145.000000   \n",
       "\n",
       "        main.disorder    specific.disorder  AB.A.delta.a.FP1  \\\n",
       "count             828                  828        828.000000   \n",
       "unique              7                   12               NaN   \n",
       "top     Mood disorder  Depressive disorder               NaN   \n",
       "freq              246                  188               NaN   \n",
       "mean              NaN                  NaN         20.367918   \n",
       "std               NaN                  NaN         11.537351   \n",
       "min               NaN                  NaN          3.272260   \n",
       "25%               NaN                  NaN         12.788493   \n",
       "50%               NaN                  NaN         17.107741   \n",
       "75%               NaN                  NaN         24.816483   \n",
       "max               NaN                  NaN         92.826192   \n",
       "\n",
       "        AB.A.delta.b.FP2  ...  COH.F.gamma.o.Pz.p.P4  COH.F.gamma.o.Pz.q.T6  \\\n",
       "count         828.000000  ...             828.000000             828.000000   \n",
       "unique               NaN  ...                    NaN                    NaN   \n",
       "top                  NaN  ...                    NaN                    NaN   \n",
       "freq                 NaN  ...                    NaN                    NaN   \n",
       "mean           21.375176  ...              75.972853              55.115058   \n",
       "std            12.540608  ...              15.757074              19.580062   \n",
       "min             3.244199  ...              12.611954               0.519048   \n",
       "25%            13.003745  ...              68.803249              41.659341   \n",
       "50%            17.877864  ...              78.762700              55.368414   \n",
       "75%            26.193861  ...              87.191115              69.666736   \n",
       "max           101.515687  ...              99.603886             100.000000   \n",
       "\n",
       "        COH.F.gamma.o.Pz.r.O1  COH.F.gamma.o.Pz.s.O2  COH.F.gamma.p.P4.q.T6  \\\n",
       "count              828.000000             828.000000             828.000000   \n",
       "unique                    NaN                    NaN                    NaN   \n",
       "top                       NaN                    NaN                    NaN   \n",
       "freq                      NaN                    NaN                    NaN   \n",
       "mean                56.993255              60.662168              70.025798   \n",
       "std                 18.261624              17.843551              17.700283   \n",
       "min                  6.926792               2.214436               2.421748   \n",
       "25%                 43.927332              48.780868              59.579014   \n",
       "50%                 56.694682              61.243097              72.473002   \n",
       "75%                 70.689085              73.797222              83.410553   \n",
       "max                 99.307895             100.000000              99.581629   \n",
       "\n",
       "        COH.F.gamma.p.P4.r.O1  COH.F.gamma.p.P4.s.O2  COH.F.gamma.q.T6.r.O1  \\\n",
       "count              828.000000             828.000000             828.000000   \n",
       "unique                    NaN                    NaN                    NaN   \n",
       "top                       NaN                    NaN                    NaN   \n",
       "freq                      NaN                    NaN                    NaN   \n",
       "mean                47.940380              66.904196              39.407460   \n",
       "std                 19.607299              16.808581              20.941359   \n",
       "min                  0.036664               1.032207               1.228502   \n",
       "25%                 32.655800              56.479459              22.006032   \n",
       "50%                 45.877296              68.250270              36.627840   \n",
       "75%                 62.154134              79.079525              54.169384   \n",
       "max                 98.720067              99.650154              98.413320   \n",
       "\n",
       "        COH.F.gamma.q.T6.s.O2  COH.F.gamma.r.O1.s.O2  \n",
       "count              828.000000             828.000000  \n",
       "unique                    NaN                    NaN  \n",
       "top                       NaN                    NaN  \n",
       "freq                      NaN                    NaN  \n",
       "mean                66.289039              57.021100  \n",
       "std                 17.922616              19.733269  \n",
       "min                  1.643951               4.340159  \n",
       "25%                 55.242184              43.834643  \n",
       "50%                 68.418304              57.581482  \n",
       "75%                 79.172126              71.643516  \n",
       "max                100.000000              99.287092  \n",
       "\n",
       "[11 rows x 1148 columns]"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val_clean = train_val_data.drop(index=na_index)\n",
    "train_val_clean.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but eeg.date might be useful\n",
    "\n",
    "X = train_val_clean.drop(columns=['main.disorder', 'specific.disorder', 'ID', 'eeg.date'])\n",
    "y = train_val_clean['main.disorder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bands = {\n",
    "    'delta': 'A', \n",
    "    'theta': 'B', \n",
    "    'alpha': 'C', \n",
    "    'beta': 'D', \n",
    "    'gamma': 'F'\n",
    "}\n",
    "\n",
    "selected_features = []\n",
    "\n",
    "for band_name, band_code in bands.items():\n",
    "    band_psd = [col for col in X.columns if f'AB.{band_code}' in col]\n",
    "    band_coh = [col for col in X.columns if f'COH.{band_code}' in col]\n",
    "    band_features = band_psd + band_coh\n",
    "\n",
    "    selector = SelectKBest(score_func=mutual_info_classif, k=int(0.05 * len(band_features)))\n",
    "    X_band = X[band_features]\n",
    "    selector.fit(X_band, y)\n",
    "    mask = selector.get_support()\n",
    "    selected_features += X_band.columns[mask].tolist()\n",
    "\n",
    "X_prioritized = X[selected_features]\n",
    "\n",
    "# mutual info scores for bands\n",
    "band_scores = {}\n",
    "for band_name, band_code in bands.items():\n",
    "    band_features = [col for col in X.columns if f'.{band_code}.' in col]\n",
    "    mi_scores = mutual_info_classif(X[band_features], y)\n",
    "    band_scores[band_name] = np.mean(mi_scores)\n",
    "\n",
    "plt.bar(band_scores.keys(), band_scores.values())\n",
    "plt.title(\"Discriminative Power of Frequency Bands\")\n",
    "plt.ylabel(\"Mean Mutual Information\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobe_mapping = {\n",
    "    'frontal': [('FP1','a'), ('FP2','b'), ('F3','d'), ('F4','f'), ('F7','c'), ('F8','g'), ('Fz','e')],\n",
    "    'temporal': [('T3','h'), ('T4','l'), ('T5','m'), ('T6','q')],\n",
    "    'central': [('C3','i'), ('C4','k'), ('Cz','j')],\n",
    "    'parietal': [('P3','n'), ('P4','p'), ('Pz','o')],\n",
    "    'occipital': [('O1','r'), ('O2','s')]\n",
    "}\n",
    "\n",
    "\n",
    "def compute_aggregates(X):\n",
    "    X_new = X.copy()\n",
    "    new_columns = {}\n",
    "\n",
    "    # 1. lobe-specific mean PSD\n",
    "    for band_code in bands.values():\n",
    "        for lobe, electrodes in lobe_mapping.items():\n",
    "            electrode_names = [elec[0] for elec in electrodes]\n",
    "\n",
    "            psd_cols = [col for col in X.columns \n",
    "                        if f'AB.{band_code}' in col \n",
    "                        and any(elec in col for elec in electrode_names)]\n",
    "            if psd_cols:\n",
    "                new_columns[f'lobe_{lobe}_band_{band_code}_mean_psd'] = X[psd_cols].mean(axis=1)\n",
    "\n",
    "    # 2. hemispheric asymmetry\n",
    "    for band_name, band_code in bands.items():\n",
    "        for left, right in [(('FP1','a'), ('FP2','b')), (('F3','d'), ('F4','f')), (('F4','f'), ('F8','g')), (('T3','h'), ('T4','l')), (('C3','i'), ('C4','k')), (('T5','m'), ('T6','q')), (('P3','n'), ('P4','p')), (('O1','r'), ('O2','s'))]:\n",
    "            left_col = f'AB.{band_code}.{band_name}.{left[1]}.{left[0]}'\n",
    "            right_col = f'AB.{band_code}.{band_name}.{right[1]}.{right[0]}'\n",
    "            new_columns[f'asym_{left[0]}_{right[0]}_band_{band_code}'] = (\n",
    "                X[left_col] - X[right_col]\n",
    "            ) / (X[left_col] + X[right_col] + 1e-6)\n",
    "    \n",
    "    # 3. within-lobe connectivity\n",
    "    for band_code in bands.values():\n",
    "        for lobe, electrodes in lobe_mapping.items():\n",
    "            electrode_names = [elec[0] for elec in electrodes]\n",
    "            coh_cols = [col for col in X.columns \n",
    "                        if f'COH.{band_code}' in col \n",
    "                        and any(e in col for e in electrode_names)]\n",
    "            if coh_cols:\n",
    "                new_columns[f'lobe_{lobe}_band_{band_code}_mean_coh'] = X[coh_cols].mean(axis=1)\n",
    "\n",
    "    # 4. global mean coherence for each band\n",
    "    for band_code in bands.values():\n",
    "        coh_cols = [col for col in X.columns if f'COH.{band_code}' in col]\n",
    "        if coh_cols:\n",
    "            new_columns[f'global_band_{band_code}_mean_coh'] = X[coh_cols].mean(axis=1)\n",
    "    \n",
    "    # 5. frontal vs. occipital PSD asymmetry\n",
    "    for band_code in bands.values():\n",
    "        frontal_col = f'lobe_frontal_band_{band_code}_mean_psd'\n",
    "        occipital_col = f'lobe_occipital_band_{band_code}_mean_psd'\n",
    "        if frontal_col in X_new.columns and occipital_col in X_new.columns:\n",
    "            new_columns[f'asym_frontal_occipital_band_{band_code}'] = (\n",
    "                X_new[frontal_col] - X_new[occipital_col]\n",
    "            ) / (X_new[frontal_col] + X_new[occipital_col] + 1e-6)\n",
    "    \n",
    "    \n",
    "    new_columns_df = pd.DataFrame(new_columns)\n",
    "    X_new = pd.concat([X_new, new_columns_df], axis=1)\n",
    "    \n",
    "    return X_new\n",
    "\n",
    "# pending: Critical Coherence Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Mood Disorder       0.71      1.00      0.83       177\n",
      "    Mood Disorder       0.00      0.00      0.00        72\n",
      "\n",
      "         accuracy                           0.71       249\n",
      "        macro avg       0.36      0.50      0.42       249\n",
      "     weighted avg       0.51      0.71      0.59       249\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Single Classed Classification\n",
    "X_aggregated = compute_aggregates(X)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "y_stage1 = (y == 'Mood disorder').astype(int)\n",
    "\n",
    "X_aggregated['sex'] = X_aggregated['sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aggregated, y_stage1, test_size=0.3, random_state=42)\n",
    "\n",
    "engineered_features = X_aggregated.columns[4:].tolist()\n",
    "num_features = ['age', 'education', 'IQ'] + engineered_features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "\n",
    "X_train_scaled[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "X_val_scaled[num_features] = scaler.transform(X_val[num_features])\n",
    "\n",
    "model = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    max_depth=2,\n",
    "    max_features=None,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=100,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val_scaled)\n",
    "\n",
    "print(classification_report(y_val, y_pred, target_names=['Not Mood Disorder', 'Mood Disorder']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                Addictive disorder       0.18      0.10      0.12        31\n",
      "                  Anxiety disorder       0.00      0.00      0.00        17\n",
      "                   Healthy control       0.00      0.00      0.00        20\n",
      "                     Mood disorder       0.32      0.96      0.47        49\n",
      "     Obsessive compulsive disorder       0.00      0.00      0.00         8\n",
      "                     Schizophrenia       0.00      0.00      0.00        23\n",
      "Trauma and stress related disorder       0.00      0.00      0.00        18\n",
      "\n",
      "                          accuracy                           0.30       166\n",
      "                         macro avg       0.07      0.15      0.09       166\n",
      "                      weighted avg       0.13      0.30      0.16       166\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/conda/envs/project/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# MULTI_LABEL CLASSIFICATION\n",
    "\n",
    "X_aggregated = compute_aggregates(X)\n",
    "X_aggregated['sex'] = X_aggregated['sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aggregated, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "engineered_features = X_aggregated.columns[4:].tolist()\n",
    "num_features = ['age', 'education', 'IQ'] + engineered_features\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[('num', StandardScaler(), num_features)]\n",
    ")\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "X_val_transformed = preprocessor.transform(X_val)\n",
    "\n",
    "# Grid Searched\n",
    "model = RandomForestClassifier(random_state=42, max_depth=2, max_features=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100)\n",
    "\n",
    "model.fit(X_train_transformed, y_train)\n",
    "y_pred = model.predict(X_val_transformed)\n",
    "\n",
    "print(classification_report(y_val, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_aggregated = compute_aggregates(X)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "y_stage1 = (y == 'Mood disorder').astype(int)\n",
    "\n",
    "X_aggregated['sex'] = X_aggregated['sex'].map({'M': 1, 'F': 0})\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_aggregated, y_stage1, test_size=0.3, random_state=42)\n",
    "\n",
    "engineered_features = X_aggregated.columns[4:].tolist()\n",
    "num_features = ['age', 'education', 'IQ'] + engineered_features\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_val_scaled = X_val.copy()\n",
    "\n",
    "X_train_scaled[num_features] = scaler.fit_transform(X_train[num_features])\n",
    "X_val_scaled[num_features] = scaler.transform(X_val[num_features])\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled.to_numpy(), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_scaled.to_numpy(), dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Train Loss: 0.6887 Val Loss: 0.6842 Val Acc: 0.6225\n",
      "Epoch [2/1000] Train Loss: 0.6881 Val Loss: 0.6832 Val Acc: 0.6386\n",
      "Epoch [3/1000] Train Loss: 0.6803 Val Loss: 0.6819 Val Acc: 0.6707\n",
      "Epoch [4/1000] Train Loss: 0.6788 Val Loss: 0.6809 Val Acc: 0.6667\n",
      "Epoch [5/1000] Train Loss: 0.6785 Val Loss: 0.6801 Val Acc: 0.6747\n",
      "Epoch [6/1000] Train Loss: 0.6793 Val Loss: 0.6792 Val Acc: 0.6667\n",
      "Epoch [7/1000] Train Loss: 0.6794 Val Loss: 0.6782 Val Acc: 0.6707\n",
      "Epoch [8/1000] Train Loss: 0.6797 Val Loss: 0.6772 Val Acc: 0.6707\n",
      "Epoch [9/1000] Train Loss: 0.6777 Val Loss: 0.6761 Val Acc: 0.6667\n",
      "Epoch [10/1000] Train Loss: 0.6804 Val Loss: 0.6752 Val Acc: 0.6667\n",
      "Epoch [11/1000] Train Loss: 0.6746 Val Loss: 0.6745 Val Acc: 0.6707\n",
      "Epoch [12/1000] Train Loss: 0.6648 Val Loss: 0.6736 Val Acc: 0.6787\n",
      "Epoch [13/1000] Train Loss: 0.6675 Val Loss: 0.6727 Val Acc: 0.6747\n",
      "Epoch [14/1000] Train Loss: 0.6693 Val Loss: 0.6721 Val Acc: 0.6827\n",
      "Epoch [15/1000] Train Loss: 0.6642 Val Loss: 0.6714 Val Acc: 0.6787\n",
      "Epoch [16/1000] Train Loss: 0.6672 Val Loss: 0.6707 Val Acc: 0.6747\n",
      "Epoch [17/1000] Train Loss: 0.6681 Val Loss: 0.6699 Val Acc: 0.6747\n",
      "Epoch [18/1000] Train Loss: 0.6708 Val Loss: 0.6692 Val Acc: 0.6747\n",
      "Epoch [19/1000] Train Loss: 0.6586 Val Loss: 0.6686 Val Acc: 0.6707\n",
      "Epoch [20/1000] Train Loss: 0.6635 Val Loss: 0.6680 Val Acc: 0.6747\n",
      "Epoch [21/1000] Train Loss: 0.6691 Val Loss: 0.6673 Val Acc: 0.6787\n",
      "Epoch [22/1000] Train Loss: 0.6612 Val Loss: 0.6666 Val Acc: 0.6787\n",
      "Epoch [23/1000] Train Loss: 0.6669 Val Loss: 0.6661 Val Acc: 0.6827\n",
      "Epoch [24/1000] Train Loss: 0.6672 Val Loss: 0.6653 Val Acc: 0.6827\n",
      "Epoch [25/1000] Train Loss: 0.6729 Val Loss: 0.6646 Val Acc: 0.6827\n",
      "Epoch [26/1000] Train Loss: 0.6645 Val Loss: 0.6640 Val Acc: 0.6867\n",
      "Epoch [27/1000] Train Loss: 0.6657 Val Loss: 0.6634 Val Acc: 0.6867\n",
      "Epoch [28/1000] Train Loss: 0.6631 Val Loss: 0.6628 Val Acc: 0.6908\n",
      "Epoch [29/1000] Train Loss: 0.6639 Val Loss: 0.6621 Val Acc: 0.6908\n",
      "Epoch [30/1000] Train Loss: 0.6509 Val Loss: 0.6613 Val Acc: 0.6948\n",
      "Epoch [31/1000] Train Loss: 0.6517 Val Loss: 0.6608 Val Acc: 0.6948\n",
      "Epoch [32/1000] Train Loss: 0.6574 Val Loss: 0.6599 Val Acc: 0.6988\n",
      "Epoch [33/1000] Train Loss: 0.6561 Val Loss: 0.6592 Val Acc: 0.7028\n",
      "Epoch [34/1000] Train Loss: 0.6590 Val Loss: 0.6585 Val Acc: 0.7028\n",
      "Epoch [35/1000] Train Loss: 0.6509 Val Loss: 0.6580 Val Acc: 0.6988\n",
      "Epoch [36/1000] Train Loss: 0.6579 Val Loss: 0.6573 Val Acc: 0.7028\n",
      "Epoch [37/1000] Train Loss: 0.6594 Val Loss: 0.6568 Val Acc: 0.7028\n",
      "Epoch [38/1000] Train Loss: 0.6545 Val Loss: 0.6564 Val Acc: 0.6988\n",
      "Epoch [39/1000] Train Loss: 0.6510 Val Loss: 0.6559 Val Acc: 0.6948\n",
      "Epoch [40/1000] Train Loss: 0.6572 Val Loss: 0.6555 Val Acc: 0.6908\n",
      "Epoch [41/1000] Train Loss: 0.6551 Val Loss: 0.6550 Val Acc: 0.6988\n",
      "Epoch [42/1000] Train Loss: 0.6620 Val Loss: 0.6545 Val Acc: 0.6988\n",
      "Epoch [43/1000] Train Loss: 0.6419 Val Loss: 0.6541 Val Acc: 0.6988\n",
      "Epoch [44/1000] Train Loss: 0.6471 Val Loss: 0.6535 Val Acc: 0.6988\n",
      "Epoch [45/1000] Train Loss: 0.6588 Val Loss: 0.6531 Val Acc: 0.6988\n",
      "Epoch [46/1000] Train Loss: 0.6487 Val Loss: 0.6531 Val Acc: 0.6988\n",
      "Epoch [47/1000] Train Loss: 0.6525 Val Loss: 0.6527 Val Acc: 0.6988\n",
      "Epoch [48/1000] Train Loss: 0.6548 Val Loss: 0.6523 Val Acc: 0.6988\n",
      "Epoch [49/1000] Train Loss: 0.6461 Val Loss: 0.6522 Val Acc: 0.6988\n",
      "Epoch [50/1000] Train Loss: 0.6538 Val Loss: 0.6520 Val Acc: 0.6988\n",
      "Epoch [51/1000] Train Loss: 0.6441 Val Loss: 0.6517 Val Acc: 0.6988\n",
      "Epoch [52/1000] Train Loss: 0.6386 Val Loss: 0.6513 Val Acc: 0.6988\n",
      "Epoch [53/1000] Train Loss: 0.6491 Val Loss: 0.6508 Val Acc: 0.6988\n",
      "Epoch [54/1000] Train Loss: 0.6506 Val Loss: 0.6504 Val Acc: 0.6988\n",
      "Epoch [55/1000] Train Loss: 0.6529 Val Loss: 0.6499 Val Acc: 0.6988\n",
      "Epoch [56/1000] Train Loss: 0.6463 Val Loss: 0.6494 Val Acc: 0.6988\n",
      "Epoch [57/1000] Train Loss: 0.6536 Val Loss: 0.6490 Val Acc: 0.6988\n",
      "Epoch [58/1000] Train Loss: 0.6499 Val Loss: 0.6485 Val Acc: 0.6948\n",
      "Epoch [59/1000] Train Loss: 0.6434 Val Loss: 0.6483 Val Acc: 0.6948\n",
      "Epoch [60/1000] Train Loss: 0.6341 Val Loss: 0.6481 Val Acc: 0.6948\n",
      "Epoch [61/1000] Train Loss: 0.6491 Val Loss: 0.6478 Val Acc: 0.6988\n",
      "Epoch [62/1000] Train Loss: 0.6445 Val Loss: 0.6475 Val Acc: 0.6948\n",
      "Epoch [63/1000] Train Loss: 0.6522 Val Loss: 0.6472 Val Acc: 0.6948\n",
      "Epoch [64/1000] Train Loss: 0.6365 Val Loss: 0.6469 Val Acc: 0.6908\n",
      "Epoch [65/1000] Train Loss: 0.6385 Val Loss: 0.6467 Val Acc: 0.6908\n",
      "Epoch [66/1000] Train Loss: 0.6376 Val Loss: 0.6466 Val Acc: 0.6908\n",
      "Epoch [67/1000] Train Loss: 0.6362 Val Loss: 0.6459 Val Acc: 0.6948\n",
      "Epoch [68/1000] Train Loss: 0.6335 Val Loss: 0.6456 Val Acc: 0.6988\n",
      "Epoch [69/1000] Train Loss: 0.6243 Val Loss: 0.6454 Val Acc: 0.6988\n",
      "Epoch [70/1000] Train Loss: 0.6402 Val Loss: 0.6452 Val Acc: 0.6988\n",
      "Epoch [71/1000] Train Loss: 0.6384 Val Loss: 0.6450 Val Acc: 0.6988\n",
      "Epoch [72/1000] Train Loss: 0.6372 Val Loss: 0.6446 Val Acc: 0.6988\n",
      "Epoch [73/1000] Train Loss: 0.6327 Val Loss: 0.6441 Val Acc: 0.6988\n",
      "Epoch [74/1000] Train Loss: 0.6341 Val Loss: 0.6439 Val Acc: 0.6988\n",
      "Epoch [75/1000] Train Loss: 0.6343 Val Loss: 0.6438 Val Acc: 0.6988\n",
      "Epoch [76/1000] Train Loss: 0.6368 Val Loss: 0.6436 Val Acc: 0.6988\n",
      "Epoch [77/1000] Train Loss: 0.6247 Val Loss: 0.6431 Val Acc: 0.6988\n",
      "Epoch [78/1000] Train Loss: 0.6304 Val Loss: 0.6428 Val Acc: 0.6988\n",
      "Epoch [79/1000] Train Loss: 0.6383 Val Loss: 0.6424 Val Acc: 0.6988\n",
      "Epoch [80/1000] Train Loss: 0.6380 Val Loss: 0.6421 Val Acc: 0.6988\n",
      "Epoch [81/1000] Train Loss: 0.6469 Val Loss: 0.6419 Val Acc: 0.7028\n",
      "Epoch [82/1000] Train Loss: 0.6267 Val Loss: 0.6415 Val Acc: 0.7028\n",
      "Epoch [83/1000] Train Loss: 0.6312 Val Loss: 0.6412 Val Acc: 0.7028\n",
      "Epoch [84/1000] Train Loss: 0.6295 Val Loss: 0.6409 Val Acc: 0.7028\n",
      "Epoch [85/1000] Train Loss: 0.6354 Val Loss: 0.6406 Val Acc: 0.7068\n",
      "Epoch [86/1000] Train Loss: 0.6226 Val Loss: 0.6403 Val Acc: 0.7068\n",
      "Epoch [87/1000] Train Loss: 0.6218 Val Loss: 0.6400 Val Acc: 0.7068\n",
      "Epoch [88/1000] Train Loss: 0.6205 Val Loss: 0.6397 Val Acc: 0.7068\n",
      "Epoch [89/1000] Train Loss: 0.6255 Val Loss: 0.6395 Val Acc: 0.7068\n",
      "Epoch [90/1000] Train Loss: 0.6432 Val Loss: 0.6394 Val Acc: 0.7068\n",
      "Epoch [91/1000] Train Loss: 0.6313 Val Loss: 0.6392 Val Acc: 0.7068\n",
      "Epoch [92/1000] Train Loss: 0.6293 Val Loss: 0.6391 Val Acc: 0.7068\n",
      "Epoch [93/1000] Train Loss: 0.6349 Val Loss: 0.6389 Val Acc: 0.7068\n",
      "Epoch [94/1000] Train Loss: 0.6329 Val Loss: 0.6388 Val Acc: 0.7068\n",
      "Epoch [95/1000] Train Loss: 0.6219 Val Loss: 0.6386 Val Acc: 0.7068\n",
      "Epoch [96/1000] Train Loss: 0.6340 Val Loss: 0.6383 Val Acc: 0.7068\n",
      "Epoch [97/1000] Train Loss: 0.6213 Val Loss: 0.6381 Val Acc: 0.7068\n",
      "Epoch [98/1000] Train Loss: 0.6293 Val Loss: 0.6379 Val Acc: 0.7068\n",
      "Epoch [99/1000] Train Loss: 0.6281 Val Loss: 0.6379 Val Acc: 0.7068\n",
      "Epoch [100/1000] Train Loss: 0.6314 Val Loss: 0.6382 Val Acc: 0.7068\n",
      "Epoch [101/1000] Train Loss: 0.6189 Val Loss: 0.6381 Val Acc: 0.7068\n",
      "Epoch [102/1000] Train Loss: 0.6345 Val Loss: 0.6381 Val Acc: 0.7068\n",
      "Epoch [103/1000] Train Loss: 0.6418 Val Loss: 0.6383 Val Acc: 0.7068\n",
      "Epoch [104/1000] Train Loss: 0.6407 Val Loss: 0.6380 Val Acc: 0.7068\n",
      "Epoch [105/1000] Train Loss: 0.6295 Val Loss: 0.6379 Val Acc: 0.7068\n",
      "Epoch [106/1000] Train Loss: 0.6316 Val Loss: 0.6377 Val Acc: 0.7068\n",
      "Epoch [107/1000] Train Loss: 0.6291 Val Loss: 0.6373 Val Acc: 0.7068\n",
      "Epoch [108/1000] Train Loss: 0.6297 Val Loss: 0.6371 Val Acc: 0.7068\n",
      "Epoch [109/1000] Train Loss: 0.6303 Val Loss: 0.6368 Val Acc: 0.7068\n",
      "Epoch [110/1000] Train Loss: 0.6311 Val Loss: 0.6369 Val Acc: 0.7068\n",
      "Epoch [111/1000] Train Loss: 0.6300 Val Loss: 0.6368 Val Acc: 0.7068\n",
      "Epoch [112/1000] Train Loss: 0.6198 Val Loss: 0.6365 Val Acc: 0.7068\n",
      "Epoch [113/1000] Train Loss: 0.6269 Val Loss: 0.6362 Val Acc: 0.7068\n",
      "Epoch [114/1000] Train Loss: 0.6210 Val Loss: 0.6360 Val Acc: 0.7068\n",
      "Epoch [115/1000] Train Loss: 0.6365 Val Loss: 0.6356 Val Acc: 0.7068\n",
      "Epoch [116/1000] Train Loss: 0.6217 Val Loss: 0.6354 Val Acc: 0.7068\n",
      "Epoch [117/1000] Train Loss: 0.6143 Val Loss: 0.6355 Val Acc: 0.7068\n",
      "Epoch [118/1000] Train Loss: 0.6330 Val Loss: 0.6356 Val Acc: 0.7068\n",
      "Epoch [119/1000] Train Loss: 0.6327 Val Loss: 0.6357 Val Acc: 0.7068\n",
      "Epoch [120/1000] Train Loss: 0.6230 Val Loss: 0.6355 Val Acc: 0.7068\n",
      "Epoch [121/1000] Train Loss: 0.6220 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [122/1000] Train Loss: 0.6180 Val Loss: 0.6350 Val Acc: 0.7068\n",
      "Epoch [123/1000] Train Loss: 0.6233 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [124/1000] Train Loss: 0.6242 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [125/1000] Train Loss: 0.6308 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [126/1000] Train Loss: 0.6206 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [127/1000] Train Loss: 0.6228 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [128/1000] Train Loss: 0.6139 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [129/1000] Train Loss: 0.6343 Val Loss: 0.6359 Val Acc: 0.7068\n",
      "Epoch [130/1000] Train Loss: 0.6193 Val Loss: 0.6359 Val Acc: 0.7068\n",
      "Epoch [131/1000] Train Loss: 0.6278 Val Loss: 0.6358 Val Acc: 0.7068\n",
      "Epoch [132/1000] Train Loss: 0.6135 Val Loss: 0.6354 Val Acc: 0.7068\n",
      "Epoch [133/1000] Train Loss: 0.6188 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [134/1000] Train Loss: 0.6289 Val Loss: 0.6352 Val Acc: 0.7068\n",
      "Epoch [135/1000] Train Loss: 0.6343 Val Loss: 0.6349 Val Acc: 0.7068\n",
      "Epoch [136/1000] Train Loss: 0.6108 Val Loss: 0.6347 Val Acc: 0.7068\n",
      "Epoch [137/1000] Train Loss: 0.6212 Val Loss: 0.6345 Val Acc: 0.7068\n",
      "Epoch [138/1000] Train Loss: 0.6206 Val Loss: 0.6346 Val Acc: 0.7068\n",
      "Epoch [139/1000] Train Loss: 0.6330 Val Loss: 0.6347 Val Acc: 0.7068\n",
      "Epoch [140/1000] Train Loss: 0.6154 Val Loss: 0.6346 Val Acc: 0.7068\n",
      "Epoch [141/1000] Train Loss: 0.6302 Val Loss: 0.6345 Val Acc: 0.7068\n",
      "Epoch [142/1000] Train Loss: 0.6206 Val Loss: 0.6345 Val Acc: 0.7068\n",
      "Epoch [143/1000] Train Loss: 0.6260 Val Loss: 0.6344 Val Acc: 0.7068\n",
      "Epoch [144/1000] Train Loss: 0.6278 Val Loss: 0.6343 Val Acc: 0.7068\n",
      "Epoch [145/1000] Train Loss: 0.6342 Val Loss: 0.6342 Val Acc: 0.7068\n",
      "Epoch [146/1000] Train Loss: 0.6275 Val Loss: 0.6341 Val Acc: 0.7068\n",
      "Epoch [147/1000] Train Loss: 0.6263 Val Loss: 0.6339 Val Acc: 0.7068\n",
      "Epoch [148/1000] Train Loss: 0.6422 Val Loss: 0.6337 Val Acc: 0.7068\n",
      "Epoch [149/1000] Train Loss: 0.6261 Val Loss: 0.6338 Val Acc: 0.7068\n",
      "Epoch [150/1000] Train Loss: 0.6208 Val Loss: 0.6336 Val Acc: 0.7068\n",
      "Epoch [151/1000] Train Loss: 0.6203 Val Loss: 0.6334 Val Acc: 0.7068\n",
      "Epoch [152/1000] Train Loss: 0.6400 Val Loss: 0.6332 Val Acc: 0.7068\n",
      "Epoch [153/1000] Train Loss: 0.6224 Val Loss: 0.6331 Val Acc: 0.7068\n",
      "Epoch [154/1000] Train Loss: 0.6225 Val Loss: 0.6329 Val Acc: 0.7068\n",
      "Epoch [155/1000] Train Loss: 0.6201 Val Loss: 0.6323 Val Acc: 0.7068\n",
      "Epoch [156/1000] Train Loss: 0.6161 Val Loss: 0.6323 Val Acc: 0.7068\n",
      "Epoch [157/1000] Train Loss: 0.6099 Val Loss: 0.6324 Val Acc: 0.7068\n",
      "Epoch [158/1000] Train Loss: 0.6257 Val Loss: 0.6323 Val Acc: 0.7068\n",
      "Epoch [159/1000] Train Loss: 0.6085 Val Loss: 0.6322 Val Acc: 0.7068\n",
      "Epoch [160/1000] Train Loss: 0.6230 Val Loss: 0.6321 Val Acc: 0.7068\n",
      "Epoch [161/1000] Train Loss: 0.6154 Val Loss: 0.6319 Val Acc: 0.7068\n",
      "Epoch [162/1000] Train Loss: 0.6190 Val Loss: 0.6318 Val Acc: 0.7068\n",
      "Epoch [163/1000] Train Loss: 0.6283 Val Loss: 0.6316 Val Acc: 0.7068\n",
      "Epoch [164/1000] Train Loss: 0.6073 Val Loss: 0.6314 Val Acc: 0.7068\n",
      "Epoch [165/1000] Train Loss: 0.6238 Val Loss: 0.6312 Val Acc: 0.7068\n",
      "Epoch [166/1000] Train Loss: 0.6199 Val Loss: 0.6312 Val Acc: 0.7068\n",
      "Epoch [167/1000] Train Loss: 0.6188 Val Loss: 0.6311 Val Acc: 0.7068\n",
      "Epoch [168/1000] Train Loss: 0.6117 Val Loss: 0.6310 Val Acc: 0.7068\n",
      "Epoch [169/1000] Train Loss: 0.6219 Val Loss: 0.6312 Val Acc: 0.7068\n",
      "Epoch [170/1000] Train Loss: 0.6172 Val Loss: 0.6312 Val Acc: 0.7068\n",
      "Epoch [171/1000] Train Loss: 0.6091 Val Loss: 0.6312 Val Acc: 0.7068\n",
      "Epoch [172/1000] Train Loss: 0.6113 Val Loss: 0.6309 Val Acc: 0.7068\n",
      "Epoch [173/1000] Train Loss: 0.6041 Val Loss: 0.6307 Val Acc: 0.7068\n",
      "Epoch [174/1000] Train Loss: 0.6126 Val Loss: 0.6304 Val Acc: 0.7068\n",
      "Epoch [175/1000] Train Loss: 0.6229 Val Loss: 0.6303 Val Acc: 0.7068\n",
      "Epoch [176/1000] Train Loss: 0.6225 Val Loss: 0.6301 Val Acc: 0.7068\n",
      "Epoch [177/1000] Train Loss: 0.6053 Val Loss: 0.6299 Val Acc: 0.7068\n",
      "Epoch [178/1000] Train Loss: 0.6227 Val Loss: 0.6299 Val Acc: 0.7068\n",
      "Epoch [179/1000] Train Loss: 0.6137 Val Loss: 0.6298 Val Acc: 0.7068\n",
      "Epoch [180/1000] Train Loss: 0.6046 Val Loss: 0.6298 Val Acc: 0.7068\n",
      "Epoch [181/1000] Train Loss: 0.6171 Val Loss: 0.6297 Val Acc: 0.7068\n",
      "Epoch [182/1000] Train Loss: 0.6182 Val Loss: 0.6295 Val Acc: 0.7068\n",
      "Epoch [183/1000] Train Loss: 0.6234 Val Loss: 0.6293 Val Acc: 0.7068\n",
      "Epoch [184/1000] Train Loss: 0.6093 Val Loss: 0.6294 Val Acc: 0.7068\n",
      "Epoch [185/1000] Train Loss: 0.6187 Val Loss: 0.6295 Val Acc: 0.7068\n",
      "Epoch [186/1000] Train Loss: 0.6121 Val Loss: 0.6294 Val Acc: 0.7068\n",
      "Epoch [187/1000] Train Loss: 0.6198 Val Loss: 0.6292 Val Acc: 0.7068\n",
      "Epoch [188/1000] Train Loss: 0.6144 Val Loss: 0.6292 Val Acc: 0.7068\n",
      "Epoch [189/1000] Train Loss: 0.6095 Val Loss: 0.6292 Val Acc: 0.7068\n",
      "Epoch [190/1000] Train Loss: 0.6029 Val Loss: 0.6288 Val Acc: 0.7068\n",
      "Epoch [191/1000] Train Loss: 0.6051 Val Loss: 0.6285 Val Acc: 0.7068\n",
      "Epoch [192/1000] Train Loss: 0.6110 Val Loss: 0.6282 Val Acc: 0.7068\n",
      "Epoch [193/1000] Train Loss: 0.6130 Val Loss: 0.6282 Val Acc: 0.7068\n",
      "Epoch [194/1000] Train Loss: 0.6185 Val Loss: 0.6281 Val Acc: 0.7068\n",
      "Epoch [195/1000] Train Loss: 0.6024 Val Loss: 0.6281 Val Acc: 0.7068\n",
      "Epoch [196/1000] Train Loss: 0.6079 Val Loss: 0.6277 Val Acc: 0.7068\n",
      "Epoch [197/1000] Train Loss: 0.6165 Val Loss: 0.6274 Val Acc: 0.7068\n",
      "Epoch [198/1000] Train Loss: 0.6001 Val Loss: 0.6275 Val Acc: 0.7068\n",
      "Epoch [199/1000] Train Loss: 0.6177 Val Loss: 0.6275 Val Acc: 0.7068\n",
      "Epoch [200/1000] Train Loss: 0.6157 Val Loss: 0.6275 Val Acc: 0.7068\n",
      "Epoch [201/1000] Train Loss: 0.6251 Val Loss: 0.6270 Val Acc: 0.7068\n",
      "Epoch [202/1000] Train Loss: 0.6044 Val Loss: 0.6271 Val Acc: 0.7068\n",
      "Epoch [203/1000] Train Loss: 0.6023 Val Loss: 0.6269 Val Acc: 0.7068\n",
      "Epoch [204/1000] Train Loss: 0.6010 Val Loss: 0.6269 Val Acc: 0.7068\n",
      "Epoch [205/1000] Train Loss: 0.6193 Val Loss: 0.6268 Val Acc: 0.7068\n",
      "Epoch [206/1000] Train Loss: 0.6020 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [207/1000] Train Loss: 0.6077 Val Loss: 0.6265 Val Acc: 0.7068\n",
      "Epoch [208/1000] Train Loss: 0.6112 Val Loss: 0.6266 Val Acc: 0.7068\n",
      "Epoch [209/1000] Train Loss: 0.6152 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [210/1000] Train Loss: 0.5992 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [211/1000] Train Loss: 0.6142 Val Loss: 0.6268 Val Acc: 0.7068\n",
      "Epoch [212/1000] Train Loss: 0.6048 Val Loss: 0.6270 Val Acc: 0.7068\n",
      "Epoch [213/1000] Train Loss: 0.6088 Val Loss: 0.6269 Val Acc: 0.7068\n",
      "Epoch [214/1000] Train Loss: 0.6109 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [215/1000] Train Loss: 0.6039 Val Loss: 0.6266 Val Acc: 0.7068\n",
      "Epoch [216/1000] Train Loss: 0.5995 Val Loss: 0.6266 Val Acc: 0.7068\n",
      "Epoch [217/1000] Train Loss: 0.6098 Val Loss: 0.6265 Val Acc: 0.7068\n",
      "Epoch [218/1000] Train Loss: 0.5882 Val Loss: 0.6268 Val Acc: 0.7068\n",
      "Epoch [219/1000] Train Loss: 0.6026 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [220/1000] Train Loss: 0.6069 Val Loss: 0.6268 Val Acc: 0.7068\n",
      "Epoch [221/1000] Train Loss: 0.6074 Val Loss: 0.6266 Val Acc: 0.7068\n",
      "Epoch [222/1000] Train Loss: 0.6197 Val Loss: 0.6268 Val Acc: 0.7068\n",
      "Epoch [223/1000] Train Loss: 0.6062 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [224/1000] Train Loss: 0.6050 Val Loss: 0.6267 Val Acc: 0.7068\n",
      "Epoch [225/1000] Train Loss: 0.5871 Val Loss: 0.6264 Val Acc: 0.7068\n",
      "Epoch [226/1000] Train Loss: 0.5997 Val Loss: 0.6262 Val Acc: 0.7068\n",
      "Epoch [227/1000] Train Loss: 0.6277 Val Loss: 0.6264 Val Acc: 0.7068\n",
      "Epoch [228/1000] Train Loss: 0.6007 Val Loss: 0.6261 Val Acc: 0.7068\n",
      "Epoch [229/1000] Train Loss: 0.6065 Val Loss: 0.6262 Val Acc: 0.7068\n",
      "Epoch [230/1000] Train Loss: 0.5984 Val Loss: 0.6261 Val Acc: 0.7068\n",
      "Epoch [231/1000] Train Loss: 0.6005 Val Loss: 0.6261 Val Acc: 0.7068\n",
      "Epoch [232/1000] Train Loss: 0.6078 Val Loss: 0.6261 Val Acc: 0.7068\n",
      "Epoch [233/1000] Train Loss: 0.6061 Val Loss: 0.6262 Val Acc: 0.7068\n",
      "Epoch [234/1000] Train Loss: 0.6063 Val Loss: 0.6262 Val Acc: 0.7068\n",
      "Epoch [235/1000] Train Loss: 0.6077 Val Loss: 0.6263 Val Acc: 0.7068\n",
      "Epoch [236/1000] Train Loss: 0.6035 Val Loss: 0.6261 Val Acc: 0.7068\n",
      "Epoch [237/1000] Train Loss: 0.5924 Val Loss: 0.6257 Val Acc: 0.7068\n",
      "Epoch [238/1000] Train Loss: 0.5939 Val Loss: 0.6257 Val Acc: 0.7068\n",
      "Epoch [239/1000] Train Loss: 0.5976 Val Loss: 0.6255 Val Acc: 0.7068\n",
      "Epoch [240/1000] Train Loss: 0.6059 Val Loss: 0.6253 Val Acc: 0.7068\n",
      "Epoch [241/1000] Train Loss: 0.5987 Val Loss: 0.6252 Val Acc: 0.7068\n",
      "Epoch [242/1000] Train Loss: 0.5954 Val Loss: 0.6251 Val Acc: 0.7068\n",
      "Epoch [243/1000] Train Loss: 0.6113 Val Loss: 0.6245 Val Acc: 0.7068\n",
      "Epoch [244/1000] Train Loss: 0.6030 Val Loss: 0.6246 Val Acc: 0.7068\n",
      "Epoch [245/1000] Train Loss: 0.5994 Val Loss: 0.6246 Val Acc: 0.7068\n",
      "Epoch [246/1000] Train Loss: 0.6020 Val Loss: 0.6245 Val Acc: 0.7068\n",
      "Epoch [247/1000] Train Loss: 0.6024 Val Loss: 0.6245 Val Acc: 0.7068\n",
      "Epoch [248/1000] Train Loss: 0.5946 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [249/1000] Train Loss: 0.6079 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [250/1000] Train Loss: 0.5989 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [251/1000] Train Loss: 0.6076 Val Loss: 0.6243 Val Acc: 0.7068\n",
      "Epoch [252/1000] Train Loss: 0.5988 Val Loss: 0.6243 Val Acc: 0.7068\n",
      "Epoch [253/1000] Train Loss: 0.5973 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [254/1000] Train Loss: 0.6004 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [255/1000] Train Loss: 0.6215 Val Loss: 0.6243 Val Acc: 0.7068\n",
      "Epoch [256/1000] Train Loss: 0.6024 Val Loss: 0.6244 Val Acc: 0.7068\n",
      "Epoch [257/1000] Train Loss: 0.6005 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [258/1000] Train Loss: 0.5971 Val Loss: 0.6241 Val Acc: 0.7068\n",
      "Epoch [259/1000] Train Loss: 0.5997 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [260/1000] Train Loss: 0.6080 Val Loss: 0.6243 Val Acc: 0.7068\n",
      "Epoch [261/1000] Train Loss: 0.5870 Val Loss: 0.6243 Val Acc: 0.7068\n",
      "Epoch [262/1000] Train Loss: 0.6108 Val Loss: 0.6242 Val Acc: 0.7068\n",
      "Epoch [263/1000] Train Loss: 0.5895 Val Loss: 0.6240 Val Acc: 0.7068\n",
      "Epoch [264/1000] Train Loss: 0.5743 Val Loss: 0.6238 Val Acc: 0.7068\n",
      "Epoch [265/1000] Train Loss: 0.5954 Val Loss: 0.6237 Val Acc: 0.7068\n",
      "Epoch [266/1000] Train Loss: 0.5915 Val Loss: 0.6237 Val Acc: 0.7068\n",
      "Epoch [267/1000] Train Loss: 0.6096 Val Loss: 0.6240 Val Acc: 0.7068\n",
      "Epoch [268/1000] Train Loss: 0.5931 Val Loss: 0.6239 Val Acc: 0.7068\n",
      "Epoch [269/1000] Train Loss: 0.5942 Val Loss: 0.6239 Val Acc: 0.7068\n",
      "Epoch [270/1000] Train Loss: 0.5985 Val Loss: 0.6239 Val Acc: 0.7068\n",
      "Epoch [271/1000] Train Loss: 0.5902 Val Loss: 0.6239 Val Acc: 0.7068\n",
      "Epoch [272/1000] Train Loss: 0.6012 Val Loss: 0.6236 Val Acc: 0.7068\n",
      "Epoch [273/1000] Train Loss: 0.5915 Val Loss: 0.6234 Val Acc: 0.7068\n",
      "Epoch [274/1000] Train Loss: 0.5963 Val Loss: 0.6232 Val Acc: 0.7068\n",
      "Epoch [275/1000] Train Loss: 0.5987 Val Loss: 0.6233 Val Acc: 0.7068\n",
      "Epoch [276/1000] Train Loss: 0.5877 Val Loss: 0.6234 Val Acc: 0.7068\n",
      "Epoch [277/1000] Train Loss: 0.6031 Val Loss: 0.6234 Val Acc: 0.7068\n",
      "Epoch [278/1000] Train Loss: 0.5990 Val Loss: 0.6234 Val Acc: 0.7068\n",
      "Epoch [279/1000] Train Loss: 0.6018 Val Loss: 0.6232 Val Acc: 0.7068\n",
      "Epoch [280/1000] Train Loss: 0.5893 Val Loss: 0.6230 Val Acc: 0.7068\n",
      "Epoch [281/1000] Train Loss: 0.5965 Val Loss: 0.6227 Val Acc: 0.7068\n",
      "Epoch [282/1000] Train Loss: 0.5961 Val Loss: 0.6228 Val Acc: 0.7068\n",
      "Epoch [283/1000] Train Loss: 0.5859 Val Loss: 0.6229 Val Acc: 0.7068\n",
      "Epoch [284/1000] Train Loss: 0.6046 Val Loss: 0.6229 Val Acc: 0.7068\n",
      "Epoch [285/1000] Train Loss: 0.5978 Val Loss: 0.6227 Val Acc: 0.7068\n",
      "Epoch [286/1000] Train Loss: 0.6015 Val Loss: 0.6227 Val Acc: 0.7068\n",
      "Epoch [287/1000] Train Loss: 0.5814 Val Loss: 0.6229 Val Acc: 0.7068\n",
      "Epoch [288/1000] Train Loss: 0.5942 Val Loss: 0.6231 Val Acc: 0.7068\n",
      "Epoch [289/1000] Train Loss: 0.6023 Val Loss: 0.6229 Val Acc: 0.7068\n",
      "Epoch [290/1000] Train Loss: 0.5889 Val Loss: 0.6227 Val Acc: 0.7068\n",
      "Epoch [291/1000] Train Loss: 0.5968 Val Loss: 0.6224 Val Acc: 0.7068\n",
      "Epoch [292/1000] Train Loss: 0.5874 Val Loss: 0.6226 Val Acc: 0.7068\n",
      "Epoch [293/1000] Train Loss: 0.5853 Val Loss: 0.6226 Val Acc: 0.7068\n",
      "Epoch [294/1000] Train Loss: 0.5990 Val Loss: 0.6221 Val Acc: 0.7068\n",
      "Epoch [295/1000] Train Loss: 0.5844 Val Loss: 0.6219 Val Acc: 0.7068\n",
      "Epoch [296/1000] Train Loss: 0.5907 Val Loss: 0.6216 Val Acc: 0.7068\n",
      "Epoch [297/1000] Train Loss: 0.5898 Val Loss: 0.6215 Val Acc: 0.7068\n",
      "Epoch [298/1000] Train Loss: 0.5904 Val Loss: 0.6215 Val Acc: 0.7068\n",
      "Epoch [299/1000] Train Loss: 0.5963 Val Loss: 0.6216 Val Acc: 0.7068\n",
      "Epoch [300/1000] Train Loss: 0.5913 Val Loss: 0.6214 Val Acc: 0.7068\n",
      "Epoch [301/1000] Train Loss: 0.5884 Val Loss: 0.6215 Val Acc: 0.7068\n",
      "Epoch [302/1000] Train Loss: 0.5930 Val Loss: 0.6213 Val Acc: 0.7068\n",
      "Epoch [303/1000] Train Loss: 0.5850 Val Loss: 0.6214 Val Acc: 0.7068\n",
      "Epoch [304/1000] Train Loss: 0.5930 Val Loss: 0.6217 Val Acc: 0.7068\n",
      "Epoch [305/1000] Train Loss: 0.5897 Val Loss: 0.6217 Val Acc: 0.7068\n",
      "Epoch [306/1000] Train Loss: 0.5933 Val Loss: 0.6220 Val Acc: 0.7068\n",
      "Epoch [307/1000] Train Loss: 0.5868 Val Loss: 0.6221 Val Acc: 0.7068\n",
      "Epoch [308/1000] Train Loss: 0.5880 Val Loss: 0.6219 Val Acc: 0.7068\n",
      "Epoch [309/1000] Train Loss: 0.6173 Val Loss: 0.6215 Val Acc: 0.7068\n",
      "Epoch [310/1000] Train Loss: 0.5920 Val Loss: 0.6216 Val Acc: 0.7068\n",
      "Epoch [311/1000] Train Loss: 0.6016 Val Loss: 0.6217 Val Acc: 0.7068\n",
      "Epoch [312/1000] Train Loss: 0.6012 Val Loss: 0.6217 Val Acc: 0.7068\n",
      "Epoch [313/1000] Train Loss: 0.6019 Val Loss: 0.6217 Val Acc: 0.7068\n",
      "Epoch [314/1000] Train Loss: 0.5799 Val Loss: 0.6212 Val Acc: 0.7068\n",
      "Epoch [315/1000] Train Loss: 0.5898 Val Loss: 0.6211 Val Acc: 0.7068\n",
      "Epoch [316/1000] Train Loss: 0.5890 Val Loss: 0.6209 Val Acc: 0.7068\n",
      "Epoch [317/1000] Train Loss: 0.5942 Val Loss: 0.6208 Val Acc: 0.7068\n",
      "Epoch [318/1000] Train Loss: 0.5922 Val Loss: 0.6206 Val Acc: 0.7068\n",
      "Epoch [319/1000] Train Loss: 0.5946 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [320/1000] Train Loss: 0.5820 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [321/1000] Train Loss: 0.5729 Val Loss: 0.6200 Val Acc: 0.7068\n",
      "Epoch [322/1000] Train Loss: 0.5822 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [323/1000] Train Loss: 0.5853 Val Loss: 0.6201 Val Acc: 0.7068\n",
      "Epoch [324/1000] Train Loss: 0.5920 Val Loss: 0.6201 Val Acc: 0.7068\n",
      "Epoch [325/1000] Train Loss: 0.5828 Val Loss: 0.6203 Val Acc: 0.7068\n",
      "Epoch [326/1000] Train Loss: 0.5849 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [327/1000] Train Loss: 0.5914 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [328/1000] Train Loss: 0.5915 Val Loss: 0.6205 Val Acc: 0.7068\n",
      "Epoch [329/1000] Train Loss: 0.5829 Val Loss: 0.6207 Val Acc: 0.7068\n",
      "Epoch [330/1000] Train Loss: 0.5831 Val Loss: 0.6208 Val Acc: 0.7068\n",
      "Epoch [331/1000] Train Loss: 0.5827 Val Loss: 0.6208 Val Acc: 0.7068\n",
      "Epoch [332/1000] Train Loss: 0.5830 Val Loss: 0.6206 Val Acc: 0.7068\n",
      "Epoch [333/1000] Train Loss: 0.5908 Val Loss: 0.6206 Val Acc: 0.7068\n",
      "Epoch [334/1000] Train Loss: 0.5902 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [335/1000] Train Loss: 0.5883 Val Loss: 0.6203 Val Acc: 0.7068\n",
      "Epoch [336/1000] Train Loss: 0.5882 Val Loss: 0.6201 Val Acc: 0.7068\n",
      "Epoch [337/1000] Train Loss: 0.5875 Val Loss: 0.6200 Val Acc: 0.7068\n",
      "Epoch [338/1000] Train Loss: 0.5866 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [339/1000] Train Loss: 0.5755 Val Loss: 0.6206 Val Acc: 0.7068\n",
      "Epoch [340/1000] Train Loss: 0.5812 Val Loss: 0.6206 Val Acc: 0.7068\n",
      "Epoch [341/1000] Train Loss: 0.5771 Val Loss: 0.6207 Val Acc: 0.7068\n",
      "Epoch [342/1000] Train Loss: 0.5875 Val Loss: 0.6205 Val Acc: 0.7068\n",
      "Epoch [343/1000] Train Loss: 0.5719 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [344/1000] Train Loss: 0.5754 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [345/1000] Train Loss: 0.5931 Val Loss: 0.6203 Val Acc: 0.7068\n",
      "Epoch [346/1000] Train Loss: 0.5748 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [347/1000] Train Loss: 0.5879 Val Loss: 0.6205 Val Acc: 0.7068\n",
      "Epoch [348/1000] Train Loss: 0.5888 Val Loss: 0.6206 Val Acc: 0.7068\n",
      "Epoch [349/1000] Train Loss: 0.5904 Val Loss: 0.6209 Val Acc: 0.7068\n",
      "Epoch [350/1000] Train Loss: 0.5773 Val Loss: 0.6208 Val Acc: 0.7068\n",
      "Epoch [351/1000] Train Loss: 0.5909 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [352/1000] Train Loss: 0.5861 Val Loss: 0.6205 Val Acc: 0.7068\n",
      "Epoch [353/1000] Train Loss: 0.5699 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [354/1000] Train Loss: 0.5884 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [355/1000] Train Loss: 0.5893 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [356/1000] Train Loss: 0.5929 Val Loss: 0.6201 Val Acc: 0.7068\n",
      "Epoch [357/1000] Train Loss: 0.5707 Val Loss: 0.6203 Val Acc: 0.7068\n",
      "Epoch [358/1000] Train Loss: 0.5686 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [359/1000] Train Loss: 0.5883 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [360/1000] Train Loss: 0.5805 Val Loss: 0.6203 Val Acc: 0.7068\n",
      "Epoch [361/1000] Train Loss: 0.5812 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [362/1000] Train Loss: 0.5844 Val Loss: 0.6200 Val Acc: 0.7068\n",
      "Epoch [363/1000] Train Loss: 0.5784 Val Loss: 0.6201 Val Acc: 0.7068\n",
      "Epoch [364/1000] Train Loss: 0.5778 Val Loss: 0.6199 Val Acc: 0.7068\n",
      "Epoch [365/1000] Train Loss: 0.5748 Val Loss: 0.6198 Val Acc: 0.7068\n",
      "Epoch [366/1000] Train Loss: 0.5903 Val Loss: 0.6197 Val Acc: 0.7068\n",
      "Epoch [367/1000] Train Loss: 0.5899 Val Loss: 0.6198 Val Acc: 0.7068\n",
      "Epoch [368/1000] Train Loss: 0.5841 Val Loss: 0.6199 Val Acc: 0.7068\n",
      "Epoch [369/1000] Train Loss: 0.5886 Val Loss: 0.6199 Val Acc: 0.7068\n",
      "Epoch [370/1000] Train Loss: 0.5844 Val Loss: 0.6199 Val Acc: 0.7068\n",
      "Epoch [371/1000] Train Loss: 0.5814 Val Loss: 0.6201 Val Acc: 0.7068\n",
      "Epoch [372/1000] Train Loss: 0.5842 Val Loss: 0.6204 Val Acc: 0.7068\n",
      "Epoch [373/1000] Train Loss: 0.5664 Val Loss: 0.6202 Val Acc: 0.7068\n",
      "Epoch [374/1000] Train Loss: 0.5847 Val Loss: 0.6193 Val Acc: 0.7068\n",
      "Epoch [375/1000] Train Loss: 0.5846 Val Loss: 0.6191 Val Acc: 0.7068\n",
      "Epoch [376/1000] Train Loss: 0.5802 Val Loss: 0.6194 Val Acc: 0.7068\n",
      "Epoch [377/1000] Train Loss: 0.5837 Val Loss: 0.6195 Val Acc: 0.7068\n",
      "Epoch [378/1000] Train Loss: 0.5718 Val Loss: 0.6194 Val Acc: 0.7068\n",
      "Epoch [379/1000] Train Loss: 0.5805 Val Loss: 0.6193 Val Acc: 0.7068\n",
      "Epoch [380/1000] Train Loss: 0.5827 Val Loss: 0.6187 Val Acc: 0.7068\n",
      "Epoch [381/1000] Train Loss: 0.5882 Val Loss: 0.6185 Val Acc: 0.7068\n",
      "Epoch [382/1000] Train Loss: 0.5637 Val Loss: 0.6183 Val Acc: 0.7068\n",
      "Epoch [383/1000] Train Loss: 0.5652 Val Loss: 0.6182 Val Acc: 0.7068\n",
      "Epoch [384/1000] Train Loss: 0.5612 Val Loss: 0.6183 Val Acc: 0.7068\n",
      "Epoch [385/1000] Train Loss: 0.5791 Val Loss: 0.6185 Val Acc: 0.7068\n",
      "Epoch [386/1000] Train Loss: 0.5746 Val Loss: 0.6184 Val Acc: 0.7068\n",
      "Epoch [387/1000] Train Loss: 0.5803 Val Loss: 0.6183 Val Acc: 0.7068\n",
      "Epoch [388/1000] Train Loss: 0.5674 Val Loss: 0.6183 Val Acc: 0.7068\n",
      "Epoch [389/1000] Train Loss: 0.5685 Val Loss: 0.6181 Val Acc: 0.7068\n",
      "Epoch [390/1000] Train Loss: 0.5706 Val Loss: 0.6184 Val Acc: 0.7068\n",
      "Epoch [391/1000] Train Loss: 0.5733 Val Loss: 0.6186 Val Acc: 0.7068\n",
      "Epoch [392/1000] Train Loss: 0.5719 Val Loss: 0.6187 Val Acc: 0.7068\n",
      "Epoch [393/1000] Train Loss: 0.5731 Val Loss: 0.6188 Val Acc: 0.7068\n",
      "Epoch [394/1000] Train Loss: 0.5980 Val Loss: 0.6192 Val Acc: 0.7068\n",
      "Epoch [395/1000] Train Loss: 0.5891 Val Loss: 0.6192 Val Acc: 0.7068\n",
      "Epoch [396/1000] Train Loss: 0.5859 Val Loss: 0.6192 Val Acc: 0.7068\n",
      "Epoch [397/1000] Train Loss: 0.5843 Val Loss: 0.6188 Val Acc: 0.7068\n",
      "Epoch [398/1000] Train Loss: 0.5763 Val Loss: 0.6189 Val Acc: 0.7068\n",
      "Epoch [399/1000] Train Loss: 0.5733 Val Loss: 0.6187 Val Acc: 0.7068\n",
      "Epoch [400/1000] Train Loss: 0.5734 Val Loss: 0.6189 Val Acc: 0.7108\n",
      "Epoch [401/1000] Train Loss: 0.5901 Val Loss: 0.6187 Val Acc: 0.7068\n",
      "Epoch [402/1000] Train Loss: 0.5678 Val Loss: 0.6188 Val Acc: 0.7068\n",
      "Epoch [403/1000] Train Loss: 0.5650 Val Loss: 0.6186 Val Acc: 0.7068\n",
      "Epoch [404/1000] Train Loss: 0.5777 Val Loss: 0.6188 Val Acc: 0.7068\n",
      "Epoch [405/1000] Train Loss: 0.5780 Val Loss: 0.6189 Val Acc: 0.7068\n",
      "Epoch [406/1000] Train Loss: 0.5721 Val Loss: 0.6191 Val Acc: 0.7068\n",
      "Epoch [407/1000] Train Loss: 0.5674 Val Loss: 0.6189 Val Acc: 0.7068\n",
      "Epoch [408/1000] Train Loss: 0.5805 Val Loss: 0.6184 Val Acc: 0.7068\n",
      "Epoch [409/1000] Train Loss: 0.5635 Val Loss: 0.6184 Val Acc: 0.7068\n",
      "Epoch [410/1000] Train Loss: 0.5833 Val Loss: 0.6180 Val Acc: 0.7068\n",
      "Epoch [411/1000] Train Loss: 0.5809 Val Loss: 0.6179 Val Acc: 0.7068\n",
      "Epoch [412/1000] Train Loss: 0.5659 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [413/1000] Train Loss: 0.5714 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [414/1000] Train Loss: 0.5762 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [415/1000] Train Loss: 0.5613 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [416/1000] Train Loss: 0.5770 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [417/1000] Train Loss: 0.5699 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [418/1000] Train Loss: 0.5818 Val Loss: 0.6178 Val Acc: 0.7068\n",
      "Epoch [419/1000] Train Loss: 0.5747 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [420/1000] Train Loss: 0.5708 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [421/1000] Train Loss: 0.5824 Val Loss: 0.6170 Val Acc: 0.7068\n",
      "Epoch [422/1000] Train Loss: 0.5754 Val Loss: 0.6169 Val Acc: 0.7068\n",
      "Epoch [423/1000] Train Loss: 0.5684 Val Loss: 0.6165 Val Acc: 0.7068\n",
      "Epoch [424/1000] Train Loss: 0.5718 Val Loss: 0.6162 Val Acc: 0.7068\n",
      "Epoch [425/1000] Train Loss: 0.5622 Val Loss: 0.6164 Val Acc: 0.7068\n",
      "Epoch [426/1000] Train Loss: 0.5776 Val Loss: 0.6165 Val Acc: 0.7068\n",
      "Epoch [427/1000] Train Loss: 0.5742 Val Loss: 0.6168 Val Acc: 0.7068\n",
      "Epoch [428/1000] Train Loss: 0.5740 Val Loss: 0.6171 Val Acc: 0.7068\n",
      "Epoch [429/1000] Train Loss: 0.5657 Val Loss: 0.6170 Val Acc: 0.7068\n",
      "Epoch [430/1000] Train Loss: 0.5640 Val Loss: 0.6171 Val Acc: 0.7068\n",
      "Epoch [431/1000] Train Loss: 0.5865 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [432/1000] Train Loss: 0.5774 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [433/1000] Train Loss: 0.5693 Val Loss: 0.6176 Val Acc: 0.7068\n",
      "Epoch [434/1000] Train Loss: 0.5712 Val Loss: 0.6181 Val Acc: 0.7068\n",
      "Epoch [435/1000] Train Loss: 0.5583 Val Loss: 0.6181 Val Acc: 0.7068\n",
      "Epoch [436/1000] Train Loss: 0.5798 Val Loss: 0.6180 Val Acc: 0.7068\n",
      "Epoch [437/1000] Train Loss: 0.5724 Val Loss: 0.6180 Val Acc: 0.7068\n",
      "Epoch [438/1000] Train Loss: 0.5521 Val Loss: 0.6177 Val Acc: 0.7068\n",
      "Epoch [439/1000] Train Loss: 0.5770 Val Loss: 0.6177 Val Acc: 0.7068\n",
      "Epoch [440/1000] Train Loss: 0.5588 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [441/1000] Train Loss: 0.5760 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [442/1000] Train Loss: 0.5649 Val Loss: 0.6177 Val Acc: 0.7068\n",
      "Epoch [443/1000] Train Loss: 0.5538 Val Loss: 0.6176 Val Acc: 0.7068\n",
      "Epoch [444/1000] Train Loss: 0.5697 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [445/1000] Train Loss: 0.5788 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [446/1000] Train Loss: 0.5747 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [447/1000] Train Loss: 0.5671 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [448/1000] Train Loss: 0.5600 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [449/1000] Train Loss: 0.5720 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [450/1000] Train Loss: 0.5647 Val Loss: 0.6170 Val Acc: 0.7068\n",
      "Epoch [451/1000] Train Loss: 0.5618 Val Loss: 0.6168 Val Acc: 0.7068\n",
      "Epoch [452/1000] Train Loss: 0.5722 Val Loss: 0.6168 Val Acc: 0.7068\n",
      "Epoch [453/1000] Train Loss: 0.5742 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [454/1000] Train Loss: 0.5681 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [455/1000] Train Loss: 0.5667 Val Loss: 0.6179 Val Acc: 0.7068\n",
      "Epoch [456/1000] Train Loss: 0.5581 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [457/1000] Train Loss: 0.5681 Val Loss: 0.6173 Val Acc: 0.7068\n",
      "Epoch [458/1000] Train Loss: 0.5736 Val Loss: 0.6171 Val Acc: 0.7068\n",
      "Epoch [459/1000] Train Loss: 0.5566 Val Loss: 0.6172 Val Acc: 0.7068\n",
      "Epoch [460/1000] Train Loss: 0.5777 Val Loss: 0.6169 Val Acc: 0.7068\n",
      "Epoch [461/1000] Train Loss: 0.5650 Val Loss: 0.6169 Val Acc: 0.7068\n",
      "Epoch [462/1000] Train Loss: 0.5617 Val Loss: 0.6164 Val Acc: 0.7068\n",
      "Epoch [463/1000] Train Loss: 0.5724 Val Loss: 0.6163 Val Acc: 0.7068\n",
      "Epoch [464/1000] Train Loss: 0.5879 Val Loss: 0.6160 Val Acc: 0.7068\n",
      "Epoch [465/1000] Train Loss: 0.5745 Val Loss: 0.6160 Val Acc: 0.7068\n",
      "Epoch [466/1000] Train Loss: 0.5604 Val Loss: 0.6162 Val Acc: 0.7068\n",
      "Epoch [467/1000] Train Loss: 0.5548 Val Loss: 0.6164 Val Acc: 0.7068\n",
      "Epoch [468/1000] Train Loss: 0.5692 Val Loss: 0.6165 Val Acc: 0.7068\n",
      "Epoch [469/1000] Train Loss: 0.5683 Val Loss: 0.6166 Val Acc: 0.7068\n",
      "Epoch [470/1000] Train Loss: 0.5434 Val Loss: 0.6169 Val Acc: 0.7068\n",
      "Epoch [471/1000] Train Loss: 0.5590 Val Loss: 0.6172 Val Acc: 0.7068\n",
      "Epoch [472/1000] Train Loss: 0.5684 Val Loss: 0.6170 Val Acc: 0.7068\n",
      "Epoch [473/1000] Train Loss: 0.5654 Val Loss: 0.6167 Val Acc: 0.7068\n",
      "Epoch [474/1000] Train Loss: 0.5535 Val Loss: 0.6160 Val Acc: 0.7068\n",
      "Epoch [475/1000] Train Loss: 0.5769 Val Loss: 0.6160 Val Acc: 0.7068\n",
      "Epoch [476/1000] Train Loss: 0.5542 Val Loss: 0.6164 Val Acc: 0.7068\n",
      "Epoch [477/1000] Train Loss: 0.5688 Val Loss: 0.6165 Val Acc: 0.7068\n",
      "Epoch [478/1000] Train Loss: 0.5668 Val Loss: 0.6170 Val Acc: 0.7028\n",
      "Epoch [479/1000] Train Loss: 0.5709 Val Loss: 0.6166 Val Acc: 0.7028\n",
      "Epoch [480/1000] Train Loss: 0.5497 Val Loss: 0.6168 Val Acc: 0.7028\n",
      "Epoch [481/1000] Train Loss: 0.5686 Val Loss: 0.6170 Val Acc: 0.7028\n",
      "Epoch [482/1000] Train Loss: 0.5559 Val Loss: 0.6171 Val Acc: 0.7028\n",
      "Epoch [483/1000] Train Loss: 0.5685 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [484/1000] Train Loss: 0.5684 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [485/1000] Train Loss: 0.5566 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [486/1000] Train Loss: 0.5422 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [487/1000] Train Loss: 0.5695 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [488/1000] Train Loss: 0.5619 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [489/1000] Train Loss: 0.5644 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [490/1000] Train Loss: 0.5503 Val Loss: 0.6177 Val Acc: 0.7028\n",
      "Epoch [491/1000] Train Loss: 0.5640 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [492/1000] Train Loss: 0.5494 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [493/1000] Train Loss: 0.5651 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [494/1000] Train Loss: 0.5641 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [495/1000] Train Loss: 0.5495 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [496/1000] Train Loss: 0.5635 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [497/1000] Train Loss: 0.5485 Val Loss: 0.6168 Val Acc: 0.7028\n",
      "Epoch [498/1000] Train Loss: 0.5677 Val Loss: 0.6168 Val Acc: 0.7028\n",
      "Epoch [499/1000] Train Loss: 0.5565 Val Loss: 0.6172 Val Acc: 0.7028\n",
      "Epoch [500/1000] Train Loss: 0.5642 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [501/1000] Train Loss: 0.5524 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [502/1000] Train Loss: 0.5536 Val Loss: 0.6176 Val Acc: 0.7068\n",
      "Epoch [503/1000] Train Loss: 0.5612 Val Loss: 0.6179 Val Acc: 0.7068\n",
      "Epoch [504/1000] Train Loss: 0.5536 Val Loss: 0.6184 Val Acc: 0.7028\n",
      "Epoch [505/1000] Train Loss: 0.5601 Val Loss: 0.6188 Val Acc: 0.7028\n",
      "Epoch [506/1000] Train Loss: 0.5600 Val Loss: 0.6188 Val Acc: 0.7028\n",
      "Epoch [507/1000] Train Loss: 0.5618 Val Loss: 0.6188 Val Acc: 0.7028\n",
      "Epoch [508/1000] Train Loss: 0.5609 Val Loss: 0.6188 Val Acc: 0.7028\n",
      "Epoch [509/1000] Train Loss: 0.5571 Val Loss: 0.6185 Val Acc: 0.7028\n",
      "Epoch [510/1000] Train Loss: 0.5626 Val Loss: 0.6184 Val Acc: 0.7028\n",
      "Epoch [511/1000] Train Loss: 0.5491 Val Loss: 0.6180 Val Acc: 0.7028\n",
      "Epoch [512/1000] Train Loss: 0.5631 Val Loss: 0.6175 Val Acc: 0.7068\n",
      "Epoch [513/1000] Train Loss: 0.5544 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [514/1000] Train Loss: 0.5518 Val Loss: 0.6174 Val Acc: 0.7068\n",
      "Epoch [515/1000] Train Loss: 0.5615 Val Loss: 0.6172 Val Acc: 0.7068\n",
      "Epoch [516/1000] Train Loss: 0.5559 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [517/1000] Train Loss: 0.5461 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [518/1000] Train Loss: 0.5554 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [519/1000] Train Loss: 0.5527 Val Loss: 0.6174 Val Acc: 0.7028\n",
      "Epoch [520/1000] Train Loss: 0.5507 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [521/1000] Train Loss: 0.5635 Val Loss: 0.6178 Val Acc: 0.7028\n",
      "Epoch [522/1000] Train Loss: 0.5649 Val Loss: 0.6177 Val Acc: 0.7028\n",
      "Epoch [523/1000] Train Loss: 0.5650 Val Loss: 0.6178 Val Acc: 0.7028\n",
      "Epoch [524/1000] Train Loss: 0.5488 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [525/1000] Train Loss: 0.5517 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [526/1000] Train Loss: 0.5563 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [527/1000] Train Loss: 0.5497 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [528/1000] Train Loss: 0.5514 Val Loss: 0.6177 Val Acc: 0.7028\n",
      "Epoch [529/1000] Train Loss: 0.5644 Val Loss: 0.6177 Val Acc: 0.7028\n",
      "Epoch [530/1000] Train Loss: 0.5596 Val Loss: 0.6173 Val Acc: 0.7028\n",
      "Epoch [531/1000] Train Loss: 0.5590 Val Loss: 0.6169 Val Acc: 0.7068\n",
      "Epoch [532/1000] Train Loss: 0.5409 Val Loss: 0.6168 Val Acc: 0.7068\n",
      "Epoch [533/1000] Train Loss: 0.5541 Val Loss: 0.6167 Val Acc: 0.7028\n",
      "Epoch [534/1000] Train Loss: 0.5515 Val Loss: 0.6169 Val Acc: 0.7028\n",
      "Epoch [535/1000] Train Loss: 0.5508 Val Loss: 0.6170 Val Acc: 0.7028\n",
      "Epoch [536/1000] Train Loss: 0.5539 Val Loss: 0.6170 Val Acc: 0.7028\n",
      "Epoch [537/1000] Train Loss: 0.5572 Val Loss: 0.6168 Val Acc: 0.7068\n",
      "Epoch [538/1000] Train Loss: 0.5582 Val Loss: 0.6168 Val Acc: 0.7028\n",
      "Epoch [539/1000] Train Loss: 0.5538 Val Loss: 0.6169 Val Acc: 0.7028\n",
      "Epoch [540/1000] Train Loss: 0.5660 Val Loss: 0.6170 Val Acc: 0.7028\n",
      "Epoch [541/1000] Train Loss: 0.5527 Val Loss: 0.6170 Val Acc: 0.7028\n",
      "Epoch [542/1000] Train Loss: 0.5433 Val Loss: 0.6169 Val Acc: 0.7028\n",
      "Epoch [543/1000] Train Loss: 0.5493 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [544/1000] Train Loss: 0.5429 Val Loss: 0.6180 Val Acc: 0.7028\n",
      "Epoch [545/1000] Train Loss: 0.5588 Val Loss: 0.6183 Val Acc: 0.7028\n",
      "Epoch [546/1000] Train Loss: 0.5562 Val Loss: 0.6185 Val Acc: 0.7028\n",
      "Epoch [547/1000] Train Loss: 0.5569 Val Loss: 0.6181 Val Acc: 0.7028\n",
      "Epoch [548/1000] Train Loss: 0.5461 Val Loss: 0.6178 Val Acc: 0.7028\n",
      "Epoch [549/1000] Train Loss: 0.5432 Val Loss: 0.6181 Val Acc: 0.7028\n",
      "Epoch [550/1000] Train Loss: 0.5436 Val Loss: 0.6175 Val Acc: 0.7028\n",
      "Epoch [551/1000] Train Loss: 0.5393 Val Loss: 0.6177 Val Acc: 0.7028\n",
      "Epoch [552/1000] Train Loss: 0.5425 Val Loss: 0.6176 Val Acc: 0.7028\n",
      "Epoch [553/1000] Train Loss: 0.5560 Val Loss: 0.6181 Val Acc: 0.7028\n",
      "Epoch [554/1000] Train Loss: 0.5510 Val Loss: 0.6182 Val Acc: 0.7028\n",
      "Epoch [555/1000] Train Loss: 0.5497 Val Loss: 0.6181 Val Acc: 0.7028\n",
      "Epoch [556/1000] Train Loss: 0.5559 Val Loss: 0.6178 Val Acc: 0.7028\n",
      "Epoch [557/1000] Train Loss: 0.5549 Val Loss: 0.6180 Val Acc: 0.7028\n",
      "Epoch [558/1000] Train Loss: 0.5574 Val Loss: 0.6183 Val Acc: 0.7028\n",
      "Epoch [559/1000] Train Loss: 0.5554 Val Loss: 0.6181 Val Acc: 0.7028\n",
      "Epoch [560/1000] Train Loss: 0.5374 Val Loss: 0.6184 Val Acc: 0.7028\n",
      "Epoch [561/1000] Train Loss: 0.5533 Val Loss: 0.6185 Val Acc: 0.7028\n",
      "Epoch [562/1000] Train Loss: 0.5489 Val Loss: 0.6187 Val Acc: 0.7028\n",
      "Epoch [563/1000] Train Loss: 0.5449 Val Loss: 0.6184 Val Acc: 0.7028\n",
      "Epoch [564/1000] Train Loss: 0.5393 Val Loss: 0.6185 Val Acc: 0.7028\n",
      "Epoch [565/1000] Train Loss: 0.5534 Val Loss: 0.6184 Val Acc: 0.7028\n",
      "Epoch [566/1000] Train Loss: 0.5397 Val Loss: 0.6194 Val Acc: 0.7028\n",
      "Epoch [567/1000] Train Loss: 0.5376 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [568/1000] Train Loss: 0.5556 Val Loss: 0.6200 Val Acc: 0.7028\n",
      "Epoch [569/1000] Train Loss: 0.5477 Val Loss: 0.6202 Val Acc: 0.7028\n",
      "Epoch [570/1000] Train Loss: 0.5412 Val Loss: 0.6202 Val Acc: 0.7028\n",
      "Epoch [571/1000] Train Loss: 0.5487 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [572/1000] Train Loss: 0.5259 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [573/1000] Train Loss: 0.5604 Val Loss: 0.6201 Val Acc: 0.7028\n",
      "Epoch [574/1000] Train Loss: 0.5483 Val Loss: 0.6203 Val Acc: 0.7028\n",
      "Epoch [575/1000] Train Loss: 0.5471 Val Loss: 0.6204 Val Acc: 0.7028\n",
      "Epoch [576/1000] Train Loss: 0.5443 Val Loss: 0.6202 Val Acc: 0.7028\n",
      "Epoch [577/1000] Train Loss: 0.5443 Val Loss: 0.6201 Val Acc: 0.7028\n",
      "Epoch [578/1000] Train Loss: 0.5485 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [579/1000] Train Loss: 0.5591 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [580/1000] Train Loss: 0.5403 Val Loss: 0.6200 Val Acc: 0.7028\n",
      "Epoch [581/1000] Train Loss: 0.5504 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [582/1000] Train Loss: 0.5483 Val Loss: 0.6201 Val Acc: 0.7028\n",
      "Epoch [583/1000] Train Loss: 0.5533 Val Loss: 0.6199 Val Acc: 0.7028\n",
      "Epoch [584/1000] Train Loss: 0.5510 Val Loss: 0.6200 Val Acc: 0.7028\n",
      "Epoch [585/1000] Train Loss: 0.5448 Val Loss: 0.6203 Val Acc: 0.7028\n",
      "Epoch [586/1000] Train Loss: 0.5535 Val Loss: 0.6206 Val Acc: 0.7028\n",
      "Epoch [587/1000] Train Loss: 0.5466 Val Loss: 0.6204 Val Acc: 0.7028\n",
      "Epoch [588/1000] Train Loss: 0.5284 Val Loss: 0.6207 Val Acc: 0.7028\n",
      "Epoch [589/1000] Train Loss: 0.5392 Val Loss: 0.6209 Val Acc: 0.7028\n",
      "Epoch [590/1000] Train Loss: 0.5472 Val Loss: 0.6207 Val Acc: 0.7028\n",
      "Epoch [591/1000] Train Loss: 0.5395 Val Loss: 0.6209 Val Acc: 0.7028\n",
      "Epoch [592/1000] Train Loss: 0.5508 Val Loss: 0.6205 Val Acc: 0.7028\n",
      "Epoch [593/1000] Train Loss: 0.5406 Val Loss: 0.6204 Val Acc: 0.7028\n",
      "Epoch [594/1000] Train Loss: 0.5360 Val Loss: 0.6205 Val Acc: 0.7028\n",
      "Epoch [595/1000] Train Loss: 0.5446 Val Loss: 0.6207 Val Acc: 0.7028\n",
      "Epoch [596/1000] Train Loss: 0.5424 Val Loss: 0.6202 Val Acc: 0.7028\n",
      "Epoch [597/1000] Train Loss: 0.5399 Val Loss: 0.6198 Val Acc: 0.7028\n",
      "Epoch [598/1000] Train Loss: 0.5375 Val Loss: 0.6196 Val Acc: 0.7028\n",
      "Epoch [599/1000] Train Loss: 0.5513 Val Loss: 0.6194 Val Acc: 0.7028\n",
      "Epoch [600/1000] Train Loss: 0.5408 Val Loss: 0.6196 Val Acc: 0.7028\n",
      "Epoch [601/1000] Train Loss: 0.5433 Val Loss: 0.6202 Val Acc: 0.7028\n",
      "Epoch [602/1000] Train Loss: 0.5392 Val Loss: 0.6204 Val Acc: 0.7028\n",
      "Epoch [603/1000] Train Loss: 0.5348 Val Loss: 0.6206 Val Acc: 0.7028\n",
      "Epoch [604/1000] Train Loss: 0.5323 Val Loss: 0.6207 Val Acc: 0.7028\n",
      "Epoch [605/1000] Train Loss: 0.5553 Val Loss: 0.6208 Val Acc: 0.7028\n",
      "Epoch [606/1000] Train Loss: 0.5345 Val Loss: 0.6208 Val Acc: 0.7028\n",
      "Epoch [607/1000] Train Loss: 0.5395 Val Loss: 0.6208 Val Acc: 0.7028\n",
      "Epoch [608/1000] Train Loss: 0.5487 Val Loss: 0.6212 Val Acc: 0.7028\n",
      "Epoch [609/1000] Train Loss: 0.5210 Val Loss: 0.6216 Val Acc: 0.7028\n",
      "Epoch [610/1000] Train Loss: 0.5282 Val Loss: 0.6219 Val Acc: 0.7028\n",
      "Epoch [611/1000] Train Loss: 0.5384 Val Loss: 0.6219 Val Acc: 0.7028\n",
      "Epoch [612/1000] Train Loss: 0.5320 Val Loss: 0.6224 Val Acc: 0.6988\n",
      "Epoch [613/1000] Train Loss: 0.5278 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [614/1000] Train Loss: 0.5430 Val Loss: 0.6230 Val Acc: 0.6988\n",
      "Epoch [615/1000] Train Loss: 0.5395 Val Loss: 0.6231 Val Acc: 0.6988\n",
      "Epoch [616/1000] Train Loss: 0.5477 Val Loss: 0.6228 Val Acc: 0.6988\n",
      "Epoch [617/1000] Train Loss: 0.5375 Val Loss: 0.6225 Val Acc: 0.6988\n",
      "Epoch [618/1000] Train Loss: 0.5320 Val Loss: 0.6224 Val Acc: 0.6988\n",
      "Epoch [619/1000] Train Loss: 0.5491 Val Loss: 0.6224 Val Acc: 0.6988\n",
      "Epoch [620/1000] Train Loss: 0.5363 Val Loss: 0.6224 Val Acc: 0.6988\n",
      "Epoch [621/1000] Train Loss: 0.5431 Val Loss: 0.6223 Val Acc: 0.6988\n",
      "Epoch [622/1000] Train Loss: 0.5280 Val Loss: 0.6223 Val Acc: 0.6988\n",
      "Epoch [623/1000] Train Loss: 0.5328 Val Loss: 0.6223 Val Acc: 0.7028\n",
      "Epoch [624/1000] Train Loss: 0.5425 Val Loss: 0.6222 Val Acc: 0.7028\n",
      "Epoch [625/1000] Train Loss: 0.5403 Val Loss: 0.6218 Val Acc: 0.7028\n",
      "Epoch [626/1000] Train Loss: 0.5446 Val Loss: 0.6219 Val Acc: 0.7028\n",
      "Epoch [627/1000] Train Loss: 0.5354 Val Loss: 0.6220 Val Acc: 0.6988\n",
      "Epoch [628/1000] Train Loss: 0.5206 Val Loss: 0.6228 Val Acc: 0.6988\n",
      "Epoch [629/1000] Train Loss: 0.5315 Val Loss: 0.6230 Val Acc: 0.6988\n",
      "Epoch [630/1000] Train Loss: 0.5452 Val Loss: 0.6230 Val Acc: 0.6988\n",
      "Epoch [631/1000] Train Loss: 0.5343 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [632/1000] Train Loss: 0.5486 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [633/1000] Train Loss: 0.5396 Val Loss: 0.6225 Val Acc: 0.6988\n",
      "Epoch [634/1000] Train Loss: 0.5311 Val Loss: 0.6222 Val Acc: 0.6988\n",
      "Epoch [635/1000] Train Loss: 0.5462 Val Loss: 0.6220 Val Acc: 0.6988\n",
      "Epoch [636/1000] Train Loss: 0.5227 Val Loss: 0.6224 Val Acc: 0.6988\n",
      "Epoch [637/1000] Train Loss: 0.5445 Val Loss: 0.6222 Val Acc: 0.6988\n",
      "Epoch [638/1000] Train Loss: 0.5254 Val Loss: 0.6226 Val Acc: 0.6948\n",
      "Epoch [639/1000] Train Loss: 0.5390 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [640/1000] Train Loss: 0.5327 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [641/1000] Train Loss: 0.5373 Val Loss: 0.6227 Val Acc: 0.6988\n",
      "Epoch [642/1000] Train Loss: 0.5379 Val Loss: 0.6224 Val Acc: 0.6988\n",
      "Epoch [643/1000] Train Loss: 0.5416 Val Loss: 0.6222 Val Acc: 0.6988\n",
      "Epoch [644/1000] Train Loss: 0.5293 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [645/1000] Train Loss: 0.5335 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [646/1000] Train Loss: 0.5336 Val Loss: 0.6231 Val Acc: 0.6988\n",
      "Epoch [647/1000] Train Loss: 0.5433 Val Loss: 0.6234 Val Acc: 0.6948\n",
      "Epoch [648/1000] Train Loss: 0.5420 Val Loss: 0.6230 Val Acc: 0.6988\n",
      "Epoch [649/1000] Train Loss: 0.5324 Val Loss: 0.6229 Val Acc: 0.6988\n",
      "Epoch [650/1000] Train Loss: 0.5348 Val Loss: 0.6227 Val Acc: 0.6988\n",
      "Epoch [651/1000] Train Loss: 0.5228 Val Loss: 0.6219 Val Acc: 0.6988\n",
      "Epoch [652/1000] Train Loss: 0.5284 Val Loss: 0.6216 Val Acc: 0.6988\n",
      "Epoch [653/1000] Train Loss: 0.5338 Val Loss: 0.6211 Val Acc: 0.6948\n",
      "Epoch [654/1000] Train Loss: 0.5357 Val Loss: 0.6203 Val Acc: 0.6948\n",
      "Epoch [655/1000] Train Loss: 0.5388 Val Loss: 0.6208 Val Acc: 0.6948\n",
      "Epoch [656/1000] Train Loss: 0.5247 Val Loss: 0.6206 Val Acc: 0.6948\n",
      "Epoch [657/1000] Train Loss: 0.5196 Val Loss: 0.6215 Val Acc: 0.6948\n",
      "Epoch [658/1000] Train Loss: 0.5451 Val Loss: 0.6217 Val Acc: 0.6948\n",
      "Epoch [659/1000] Train Loss: 0.5385 Val Loss: 0.6219 Val Acc: 0.6948\n",
      "Epoch [660/1000] Train Loss: 0.5235 Val Loss: 0.6223 Val Acc: 0.6948\n",
      "Epoch [661/1000] Train Loss: 0.5337 Val Loss: 0.6228 Val Acc: 0.6948\n",
      "Epoch [662/1000] Train Loss: 0.5363 Val Loss: 0.6227 Val Acc: 0.6948\n",
      "Epoch [663/1000] Train Loss: 0.5256 Val Loss: 0.6218 Val Acc: 0.6948\n",
      "Epoch [664/1000] Train Loss: 0.5380 Val Loss: 0.6213 Val Acc: 0.6948\n",
      "Epoch [665/1000] Train Loss: 0.5242 Val Loss: 0.6216 Val Acc: 0.6948\n",
      "Epoch [666/1000] Train Loss: 0.5274 Val Loss: 0.6220 Val Acc: 0.6948\n",
      "Epoch [667/1000] Train Loss: 0.5452 Val Loss: 0.6224 Val Acc: 0.6948\n",
      "Epoch [668/1000] Train Loss: 0.5178 Val Loss: 0.6229 Val Acc: 0.6948\n",
      "Epoch [669/1000] Train Loss: 0.5213 Val Loss: 0.6235 Val Acc: 0.6948\n",
      "Epoch [670/1000] Train Loss: 0.5235 Val Loss: 0.6234 Val Acc: 0.6948\n",
      "Epoch [671/1000] Train Loss: 0.5393 Val Loss: 0.6230 Val Acc: 0.6948\n",
      "Epoch [672/1000] Train Loss: 0.5232 Val Loss: 0.6229 Val Acc: 0.6948\n",
      "Epoch [673/1000] Train Loss: 0.5313 Val Loss: 0.6232 Val Acc: 0.6948\n",
      "Epoch [674/1000] Train Loss: 0.5303 Val Loss: 0.6232 Val Acc: 0.6948\n",
      "Epoch [675/1000] Train Loss: 0.5319 Val Loss: 0.6230 Val Acc: 0.6948\n",
      "Epoch [676/1000] Train Loss: 0.5344 Val Loss: 0.6229 Val Acc: 0.6948\n",
      "Epoch [677/1000] Train Loss: 0.5240 Val Loss: 0.6231 Val Acc: 0.6948\n",
      "Epoch [678/1000] Train Loss: 0.5326 Val Loss: 0.6224 Val Acc: 0.6948\n",
      "Epoch [679/1000] Train Loss: 0.5352 Val Loss: 0.6213 Val Acc: 0.6948\n",
      "Epoch [680/1000] Train Loss: 0.5311 Val Loss: 0.6212 Val Acc: 0.6948\n",
      "Epoch [681/1000] Train Loss: 0.5357 Val Loss: 0.6215 Val Acc: 0.6948\n",
      "Epoch [682/1000] Train Loss: 0.5305 Val Loss: 0.6218 Val Acc: 0.6948\n",
      "Epoch [683/1000] Train Loss: 0.5277 Val Loss: 0.6223 Val Acc: 0.6948\n",
      "Epoch [684/1000] Train Loss: 0.5196 Val Loss: 0.6221 Val Acc: 0.6948\n",
      "Epoch [685/1000] Train Loss: 0.5232 Val Loss: 0.6224 Val Acc: 0.6948\n",
      "Epoch [686/1000] Train Loss: 0.5200 Val Loss: 0.6224 Val Acc: 0.6948\n",
      "Epoch [687/1000] Train Loss: 0.5246 Val Loss: 0.6225 Val Acc: 0.6948\n",
      "Epoch [688/1000] Train Loss: 0.5190 Val Loss: 0.6235 Val Acc: 0.6948\n",
      "Epoch [689/1000] Train Loss: 0.5228 Val Loss: 0.6236 Val Acc: 0.6948\n",
      "Epoch [690/1000] Train Loss: 0.5220 Val Loss: 0.6238 Val Acc: 0.6948\n",
      "Epoch [691/1000] Train Loss: 0.5263 Val Loss: 0.6234 Val Acc: 0.6948\n",
      "Epoch [692/1000] Train Loss: 0.5099 Val Loss: 0.6231 Val Acc: 0.6948\n",
      "Epoch [693/1000] Train Loss: 0.5224 Val Loss: 0.6226 Val Acc: 0.6948\n",
      "Epoch [694/1000] Train Loss: 0.5189 Val Loss: 0.6229 Val Acc: 0.6948\n",
      "Epoch [695/1000] Train Loss: 0.5249 Val Loss: 0.6231 Val Acc: 0.6948\n",
      "Epoch [696/1000] Train Loss: 0.5130 Val Loss: 0.6230 Val Acc: 0.6948\n",
      "Epoch [697/1000] Train Loss: 0.5162 Val Loss: 0.6230 Val Acc: 0.6948\n",
      "Epoch [698/1000] Train Loss: 0.5301 Val Loss: 0.6233 Val Acc: 0.6948\n",
      "Epoch [699/1000] Train Loss: 0.5171 Val Loss: 0.6231 Val Acc: 0.6948\n",
      "Epoch [700/1000] Train Loss: 0.5223 Val Loss: 0.6225 Val Acc: 0.6948\n",
      "Epoch [701/1000] Train Loss: 0.5246 Val Loss: 0.6219 Val Acc: 0.6948\n",
      "Epoch [702/1000] Train Loss: 0.5251 Val Loss: 0.6224 Val Acc: 0.6948\n",
      "Epoch [703/1000] Train Loss: 0.5202 Val Loss: 0.6231 Val Acc: 0.6948\n",
      "Epoch [704/1000] Train Loss: 0.5124 Val Loss: 0.6233 Val Acc: 0.6948\n",
      "Epoch [705/1000] Train Loss: 0.5381 Val Loss: 0.6228 Val Acc: 0.6948\n",
      "Epoch [706/1000] Train Loss: 0.5185 Val Loss: 0.6232 Val Acc: 0.6948\n",
      "Epoch [707/1000] Train Loss: 0.5215 Val Loss: 0.6233 Val Acc: 0.6948\n",
      "Epoch [708/1000] Train Loss: 0.5182 Val Loss: 0.6234 Val Acc: 0.6948\n",
      "Epoch [709/1000] Train Loss: 0.5185 Val Loss: 0.6238 Val Acc: 0.6988\n",
      "Epoch [710/1000] Train Loss: 0.5281 Val Loss: 0.6230 Val Acc: 0.6988\n",
      "Epoch [711/1000] Train Loss: 0.5143 Val Loss: 0.6230 Val Acc: 0.6988\n",
      "Epoch [712/1000] Train Loss: 0.5185 Val Loss: 0.6238 Val Acc: 0.6988\n",
      "Epoch [713/1000] Train Loss: 0.5199 Val Loss: 0.6239 Val Acc: 0.6988\n",
      "Epoch [714/1000] Train Loss: 0.5159 Val Loss: 0.6238 Val Acc: 0.6988\n",
      "Epoch [715/1000] Train Loss: 0.5203 Val Loss: 0.6246 Val Acc: 0.6988\n",
      "Epoch [716/1000] Train Loss: 0.5272 Val Loss: 0.6252 Val Acc: 0.6988\n",
      "Epoch [717/1000] Train Loss: 0.5156 Val Loss: 0.6252 Val Acc: 0.6988\n",
      "Epoch [718/1000] Train Loss: 0.5243 Val Loss: 0.6256 Val Acc: 0.6988\n",
      "Epoch [719/1000] Train Loss: 0.5368 Val Loss: 0.6258 Val Acc: 0.6988\n",
      "Epoch [720/1000] Train Loss: 0.5288 Val Loss: 0.6257 Val Acc: 0.6988\n",
      "Epoch [721/1000] Train Loss: 0.5301 Val Loss: 0.6265 Val Acc: 0.6948\n",
      "Epoch [722/1000] Train Loss: 0.5362 Val Loss: 0.6265 Val Acc: 0.6948\n",
      "Epoch [723/1000] Train Loss: 0.5189 Val Loss: 0.6267 Val Acc: 0.6948\n",
      "Epoch [724/1000] Train Loss: 0.5077 Val Loss: 0.6270 Val Acc: 0.6908\n",
      "Epoch [725/1000] Train Loss: 0.5209 Val Loss: 0.6274 Val Acc: 0.6948\n",
      "Epoch [726/1000] Train Loss: 0.5181 Val Loss: 0.6270 Val Acc: 0.6948\n",
      "Epoch [727/1000] Train Loss: 0.5194 Val Loss: 0.6267 Val Acc: 0.6948\n",
      "Epoch [728/1000] Train Loss: 0.5238 Val Loss: 0.6268 Val Acc: 0.6908\n",
      "Epoch [729/1000] Train Loss: 0.5199 Val Loss: 0.6255 Val Acc: 0.6948\n",
      "Epoch [730/1000] Train Loss: 0.5133 Val Loss: 0.6251 Val Acc: 0.6948\n",
      "Epoch [731/1000] Train Loss: 0.5344 Val Loss: 0.6255 Val Acc: 0.6948\n",
      "Epoch [732/1000] Train Loss: 0.5179 Val Loss: 0.6257 Val Acc: 0.6948\n",
      "Epoch [733/1000] Train Loss: 0.5200 Val Loss: 0.6260 Val Acc: 0.6908\n",
      "Epoch [734/1000] Train Loss: 0.5180 Val Loss: 0.6261 Val Acc: 0.6908\n",
      "Epoch [735/1000] Train Loss: 0.5239 Val Loss: 0.6265 Val Acc: 0.6908\n",
      "Epoch [736/1000] Train Loss: 0.5161 Val Loss: 0.6262 Val Acc: 0.6908\n",
      "Epoch [737/1000] Train Loss: 0.5170 Val Loss: 0.6270 Val Acc: 0.6908\n",
      "Epoch [738/1000] Train Loss: 0.5076 Val Loss: 0.6277 Val Acc: 0.6948\n",
      "Epoch [739/1000] Train Loss: 0.5183 Val Loss: 0.6288 Val Acc: 0.6988\n",
      "Epoch [740/1000] Train Loss: 0.5319 Val Loss: 0.6281 Val Acc: 0.6988\n",
      "Epoch [741/1000] Train Loss: 0.5161 Val Loss: 0.6276 Val Acc: 0.6988\n",
      "Epoch [742/1000] Train Loss: 0.5230 Val Loss: 0.6274 Val Acc: 0.6948\n",
      "Epoch [743/1000] Train Loss: 0.5024 Val Loss: 0.6272 Val Acc: 0.6948\n",
      "Epoch [744/1000] Train Loss: 0.5160 Val Loss: 0.6270 Val Acc: 0.6948\n",
      "Epoch [745/1000] Train Loss: 0.5203 Val Loss: 0.6270 Val Acc: 0.6988\n",
      "Epoch [746/1000] Train Loss: 0.5383 Val Loss: 0.6262 Val Acc: 0.6988\n",
      "Epoch [747/1000] Train Loss: 0.5072 Val Loss: 0.6263 Val Acc: 0.6988\n",
      "Epoch [748/1000] Train Loss: 0.5166 Val Loss: 0.6266 Val Acc: 0.6988\n",
      "Epoch [749/1000] Train Loss: 0.5114 Val Loss: 0.6269 Val Acc: 0.6988\n",
      "Epoch [750/1000] Train Loss: 0.5057 Val Loss: 0.6268 Val Acc: 0.6988\n",
      "Epoch [751/1000] Train Loss: 0.5092 Val Loss: 0.6269 Val Acc: 0.6988\n",
      "Epoch [752/1000] Train Loss: 0.5154 Val Loss: 0.6275 Val Acc: 0.6948\n",
      "Epoch [753/1000] Train Loss: 0.5282 Val Loss: 0.6278 Val Acc: 0.6948\n",
      "Epoch [754/1000] Train Loss: 0.5099 Val Loss: 0.6281 Val Acc: 0.6948\n",
      "Epoch [755/1000] Train Loss: 0.5100 Val Loss: 0.6279 Val Acc: 0.6948\n",
      "Epoch [756/1000] Train Loss: 0.5108 Val Loss: 0.6278 Val Acc: 0.6948\n",
      "Epoch [757/1000] Train Loss: 0.5237 Val Loss: 0.6272 Val Acc: 0.6948\n",
      "Epoch [758/1000] Train Loss: 0.5041 Val Loss: 0.6277 Val Acc: 0.6948\n",
      "Epoch [759/1000] Train Loss: 0.5139 Val Loss: 0.6280 Val Acc: 0.6948\n",
      "Epoch [760/1000] Train Loss: 0.4921 Val Loss: 0.6284 Val Acc: 0.6948\n",
      "Epoch [761/1000] Train Loss: 0.5125 Val Loss: 0.6279 Val Acc: 0.6948\n",
      "Epoch [762/1000] Train Loss: 0.5036 Val Loss: 0.6279 Val Acc: 0.6948\n",
      "Epoch [763/1000] Train Loss: 0.5056 Val Loss: 0.6285 Val Acc: 0.6948\n",
      "Epoch [764/1000] Train Loss: 0.5144 Val Loss: 0.6287 Val Acc: 0.6948\n",
      "Epoch [765/1000] Train Loss: 0.5047 Val Loss: 0.6285 Val Acc: 0.6988\n",
      "Epoch [766/1000] Train Loss: 0.5134 Val Loss: 0.6283 Val Acc: 0.6988\n",
      "Epoch [767/1000] Train Loss: 0.5144 Val Loss: 0.6286 Val Acc: 0.6988\n",
      "Epoch [768/1000] Train Loss: 0.5042 Val Loss: 0.6285 Val Acc: 0.6988\n",
      "Epoch [769/1000] Train Loss: 0.5067 Val Loss: 0.6274 Val Acc: 0.6988\n",
      "Epoch [770/1000] Train Loss: 0.5080 Val Loss: 0.6277 Val Acc: 0.6948\n",
      "Epoch [771/1000] Train Loss: 0.5132 Val Loss: 0.6283 Val Acc: 0.6988\n",
      "Epoch [772/1000] Train Loss: 0.5127 Val Loss: 0.6282 Val Acc: 0.6988\n",
      "Epoch [773/1000] Train Loss: 0.5043 Val Loss: 0.6282 Val Acc: 0.7028\n",
      "Epoch [774/1000] Train Loss: 0.5099 Val Loss: 0.6282 Val Acc: 0.6988\n",
      "Epoch [775/1000] Train Loss: 0.5129 Val Loss: 0.6283 Val Acc: 0.6988\n",
      "Epoch [776/1000] Train Loss: 0.4999 Val Loss: 0.6282 Val Acc: 0.6988\n",
      "Epoch [777/1000] Train Loss: 0.5105 Val Loss: 0.6279 Val Acc: 0.6988\n",
      "Epoch [778/1000] Train Loss: 0.5010 Val Loss: 0.6282 Val Acc: 0.6988\n",
      "Epoch [779/1000] Train Loss: 0.4910 Val Loss: 0.6285 Val Acc: 0.6988\n",
      "Epoch [780/1000] Train Loss: 0.5042 Val Loss: 0.6289 Val Acc: 0.6988\n",
      "Epoch [781/1000] Train Loss: 0.5146 Val Loss: 0.6292 Val Acc: 0.6948\n",
      "Epoch [782/1000] Train Loss: 0.5012 Val Loss: 0.6296 Val Acc: 0.6948\n",
      "Epoch [783/1000] Train Loss: 0.5174 Val Loss: 0.6296 Val Acc: 0.6948\n",
      "Epoch [784/1000] Train Loss: 0.4987 Val Loss: 0.6295 Val Acc: 0.6988\n",
      "Epoch [785/1000] Train Loss: 0.5061 Val Loss: 0.6294 Val Acc: 0.6988\n",
      "Epoch [786/1000] Train Loss: 0.4923 Val Loss: 0.6289 Val Acc: 0.6988\n",
      "Epoch [787/1000] Train Loss: 0.5015 Val Loss: 0.6292 Val Acc: 0.6988\n",
      "Epoch [788/1000] Train Loss: 0.5115 Val Loss: 0.6296 Val Acc: 0.6988\n",
      "Epoch [789/1000] Train Loss: 0.5251 Val Loss: 0.6292 Val Acc: 0.6988\n",
      "Epoch [790/1000] Train Loss: 0.5058 Val Loss: 0.6300 Val Acc: 0.6988\n",
      "Epoch [791/1000] Train Loss: 0.4919 Val Loss: 0.6312 Val Acc: 0.6988\n",
      "Epoch [792/1000] Train Loss: 0.5026 Val Loss: 0.6317 Val Acc: 0.6988\n",
      "Epoch [793/1000] Train Loss: 0.5128 Val Loss: 0.6313 Val Acc: 0.6988\n",
      "Epoch [794/1000] Train Loss: 0.5013 Val Loss: 0.6312 Val Acc: 0.6988\n",
      "Epoch [795/1000] Train Loss: 0.5049 Val Loss: 0.6310 Val Acc: 0.6988\n",
      "Epoch [796/1000] Train Loss: 0.4972 Val Loss: 0.6314 Val Acc: 0.6988\n",
      "Epoch [797/1000] Train Loss: 0.5111 Val Loss: 0.6313 Val Acc: 0.6988\n",
      "Epoch [798/1000] Train Loss: 0.4911 Val Loss: 0.6308 Val Acc: 0.6988\n",
      "Epoch [799/1000] Train Loss: 0.5101 Val Loss: 0.6306 Val Acc: 0.6988\n",
      "Epoch [800/1000] Train Loss: 0.4918 Val Loss: 0.6307 Val Acc: 0.6988\n",
      "Epoch [801/1000] Train Loss: 0.5023 Val Loss: 0.6312 Val Acc: 0.6988\n",
      "Epoch [802/1000] Train Loss: 0.4950 Val Loss: 0.6313 Val Acc: 0.6988\n",
      "Epoch [803/1000] Train Loss: 0.4940 Val Loss: 0.6315 Val Acc: 0.6988\n",
      "Epoch [804/1000] Train Loss: 0.5089 Val Loss: 0.6304 Val Acc: 0.6988\n",
      "Epoch [805/1000] Train Loss: 0.4885 Val Loss: 0.6306 Val Acc: 0.6988\n",
      "Epoch [806/1000] Train Loss: 0.5020 Val Loss: 0.6306 Val Acc: 0.6988\n",
      "Epoch [807/1000] Train Loss: 0.5055 Val Loss: 0.6308 Val Acc: 0.6988\n",
      "Epoch [808/1000] Train Loss: 0.5103 Val Loss: 0.6305 Val Acc: 0.6988\n",
      "Epoch [809/1000] Train Loss: 0.5063 Val Loss: 0.6307 Val Acc: 0.6988\n",
      "Epoch [810/1000] Train Loss: 0.5031 Val Loss: 0.6312 Val Acc: 0.6988\n",
      "Epoch [811/1000] Train Loss: 0.4946 Val Loss: 0.6319 Val Acc: 0.6988\n",
      "Epoch [812/1000] Train Loss: 0.5030 Val Loss: 0.6320 Val Acc: 0.6988\n",
      "Epoch [813/1000] Train Loss: 0.5123 Val Loss: 0.6322 Val Acc: 0.6988\n",
      "Epoch [814/1000] Train Loss: 0.4823 Val Loss: 0.6327 Val Acc: 0.6988\n",
      "Epoch [815/1000] Train Loss: 0.5013 Val Loss: 0.6331 Val Acc: 0.6988\n",
      "Epoch [816/1000] Train Loss: 0.4993 Val Loss: 0.6332 Val Acc: 0.6988\n",
      "Epoch [817/1000] Train Loss: 0.4959 Val Loss: 0.6336 Val Acc: 0.6988\n",
      "Epoch [818/1000] Train Loss: 0.4956 Val Loss: 0.6336 Val Acc: 0.6988\n",
      "Epoch [819/1000] Train Loss: 0.5064 Val Loss: 0.6337 Val Acc: 0.6988\n",
      "Epoch [820/1000] Train Loss: 0.5087 Val Loss: 0.6335 Val Acc: 0.6988\n",
      "Epoch [821/1000] Train Loss: 0.4851 Val Loss: 0.6337 Val Acc: 0.6988\n",
      "Epoch [822/1000] Train Loss: 0.5065 Val Loss: 0.6344 Val Acc: 0.6988\n",
      "Epoch [823/1000] Train Loss: 0.4863 Val Loss: 0.6350 Val Acc: 0.6988\n",
      "Epoch [824/1000] Train Loss: 0.4951 Val Loss: 0.6350 Val Acc: 0.6988\n",
      "Epoch [825/1000] Train Loss: 0.4838 Val Loss: 0.6355 Val Acc: 0.6988\n",
      "Epoch [826/1000] Train Loss: 0.4841 Val Loss: 0.6357 Val Acc: 0.6988\n",
      "Epoch [827/1000] Train Loss: 0.4972 Val Loss: 0.6344 Val Acc: 0.6988\n",
      "Epoch [828/1000] Train Loss: 0.5080 Val Loss: 0.6344 Val Acc: 0.6988\n",
      "Epoch [829/1000] Train Loss: 0.5043 Val Loss: 0.6344 Val Acc: 0.6988\n",
      "Epoch [830/1000] Train Loss: 0.5094 Val Loss: 0.6346 Val Acc: 0.6988\n",
      "Epoch [831/1000] Train Loss: 0.4908 Val Loss: 0.6339 Val Acc: 0.6988\n",
      "Epoch [832/1000] Train Loss: 0.5011 Val Loss: 0.6332 Val Acc: 0.6988\n",
      "Epoch [833/1000] Train Loss: 0.4914 Val Loss: 0.6331 Val Acc: 0.6988\n",
      "Epoch [834/1000] Train Loss: 0.5033 Val Loss: 0.6335 Val Acc: 0.6988\n",
      "Epoch [835/1000] Train Loss: 0.5070 Val Loss: 0.6339 Val Acc: 0.6988\n",
      "Epoch [836/1000] Train Loss: 0.5080 Val Loss: 0.6343 Val Acc: 0.6988\n",
      "Epoch [837/1000] Train Loss: 0.4918 Val Loss: 0.6342 Val Acc: 0.6988\n",
      "Epoch [838/1000] Train Loss: 0.4905 Val Loss: 0.6346 Val Acc: 0.6988\n",
      "Epoch [839/1000] Train Loss: 0.5053 Val Loss: 0.6340 Val Acc: 0.6988\n",
      "Epoch [840/1000] Train Loss: 0.5045 Val Loss: 0.6339 Val Acc: 0.6988\n",
      "Epoch [841/1000] Train Loss: 0.4912 Val Loss: 0.6338 Val Acc: 0.6988\n",
      "Epoch [842/1000] Train Loss: 0.4923 Val Loss: 0.6338 Val Acc: 0.6988\n",
      "Epoch [843/1000] Train Loss: 0.4837 Val Loss: 0.6343 Val Acc: 0.6988\n",
      "Epoch [844/1000] Train Loss: 0.4879 Val Loss: 0.6341 Val Acc: 0.6988\n",
      "Epoch [845/1000] Train Loss: 0.4930 Val Loss: 0.6349 Val Acc: 0.6988\n",
      "Epoch [846/1000] Train Loss: 0.4871 Val Loss: 0.6351 Val Acc: 0.6988\n",
      "Epoch [847/1000] Train Loss: 0.5079 Val Loss: 0.6353 Val Acc: 0.6988\n",
      "Epoch [848/1000] Train Loss: 0.5063 Val Loss: 0.6354 Val Acc: 0.6988\n",
      "Epoch [849/1000] Train Loss: 0.4980 Val Loss: 0.6355 Val Acc: 0.6988\n",
      "Epoch [850/1000] Train Loss: 0.4926 Val Loss: 0.6354 Val Acc: 0.6988\n",
      "Epoch [851/1000] Train Loss: 0.4914 Val Loss: 0.6359 Val Acc: 0.6988\n",
      "Epoch [852/1000] Train Loss: 0.4919 Val Loss: 0.6362 Val Acc: 0.6988\n",
      "Epoch [853/1000] Train Loss: 0.5019 Val Loss: 0.6356 Val Acc: 0.6988\n",
      "Epoch [854/1000] Train Loss: 0.5027 Val Loss: 0.6362 Val Acc: 0.6988\n",
      "Epoch [855/1000] Train Loss: 0.4927 Val Loss: 0.6361 Val Acc: 0.6988\n",
      "Epoch [856/1000] Train Loss: 0.4983 Val Loss: 0.6366 Val Acc: 0.6988\n",
      "Epoch [857/1000] Train Loss: 0.4867 Val Loss: 0.6376 Val Acc: 0.6988\n",
      "Epoch [858/1000] Train Loss: 0.4954 Val Loss: 0.6382 Val Acc: 0.6988\n",
      "Epoch [859/1000] Train Loss: 0.4855 Val Loss: 0.6379 Val Acc: 0.6988\n",
      "Epoch [860/1000] Train Loss: 0.5034 Val Loss: 0.6378 Val Acc: 0.6948\n",
      "Epoch [861/1000] Train Loss: 0.4916 Val Loss: 0.6374 Val Acc: 0.6948\n",
      "Epoch [862/1000] Train Loss: 0.4865 Val Loss: 0.6375 Val Acc: 0.6948\n",
      "Epoch [863/1000] Train Loss: 0.4960 Val Loss: 0.6367 Val Acc: 0.6948\n",
      "Epoch [864/1000] Train Loss: 0.4946 Val Loss: 0.6363 Val Acc: 0.6948\n",
      "Epoch [865/1000] Train Loss: 0.4889 Val Loss: 0.6362 Val Acc: 0.6948\n",
      "Epoch [866/1000] Train Loss: 0.4932 Val Loss: 0.6366 Val Acc: 0.6948\n",
      "Epoch [867/1000] Train Loss: 0.4980 Val Loss: 0.6361 Val Acc: 0.6948\n",
      "Epoch [868/1000] Train Loss: 0.4946 Val Loss: 0.6354 Val Acc: 0.6948\n",
      "Epoch [869/1000] Train Loss: 0.5068 Val Loss: 0.6358 Val Acc: 0.6948\n",
      "Epoch [870/1000] Train Loss: 0.4919 Val Loss: 0.6374 Val Acc: 0.6948\n",
      "Epoch [871/1000] Train Loss: 0.4809 Val Loss: 0.6376 Val Acc: 0.6948\n",
      "Epoch [872/1000] Train Loss: 0.4914 Val Loss: 0.6381 Val Acc: 0.6988\n",
      "Epoch [873/1000] Train Loss: 0.4922 Val Loss: 0.6393 Val Acc: 0.6948\n",
      "Epoch [874/1000] Train Loss: 0.4928 Val Loss: 0.6384 Val Acc: 0.6948\n",
      "Epoch [875/1000] Train Loss: 0.4898 Val Loss: 0.6384 Val Acc: 0.6948\n",
      "Epoch [876/1000] Train Loss: 0.4942 Val Loss: 0.6384 Val Acc: 0.6948\n",
      "Epoch [877/1000] Train Loss: 0.4983 Val Loss: 0.6384 Val Acc: 0.6948\n",
      "Epoch [878/1000] Train Loss: 0.4801 Val Loss: 0.6386 Val Acc: 0.6948\n",
      "Epoch [879/1000] Train Loss: 0.4952 Val Loss: 0.6385 Val Acc: 0.6948\n",
      "Epoch [880/1000] Train Loss: 0.4932 Val Loss: 0.6385 Val Acc: 0.6948\n",
      "Epoch [881/1000] Train Loss: 0.4858 Val Loss: 0.6387 Val Acc: 0.6948\n",
      "Epoch [882/1000] Train Loss: 0.4716 Val Loss: 0.6391 Val Acc: 0.6948\n",
      "Epoch [883/1000] Train Loss: 0.4840 Val Loss: 0.6393 Val Acc: 0.6948\n",
      "Epoch [884/1000] Train Loss: 0.4918 Val Loss: 0.6386 Val Acc: 0.6948\n",
      "Epoch [885/1000] Train Loss: 0.4885 Val Loss: 0.6372 Val Acc: 0.6988\n",
      "Epoch [886/1000] Train Loss: 0.4712 Val Loss: 0.6367 Val Acc: 0.6988\n",
      "Epoch [887/1000] Train Loss: 0.4958 Val Loss: 0.6370 Val Acc: 0.6988\n",
      "Epoch [888/1000] Train Loss: 0.4814 Val Loss: 0.6373 Val Acc: 0.6988\n",
      "Epoch [889/1000] Train Loss: 0.4794 Val Loss: 0.6371 Val Acc: 0.6988\n",
      "Epoch [890/1000] Train Loss: 0.4841 Val Loss: 0.6378 Val Acc: 0.6988\n",
      "Epoch [891/1000] Train Loss: 0.4747 Val Loss: 0.6386 Val Acc: 0.6988\n",
      "Epoch [892/1000] Train Loss: 0.5054 Val Loss: 0.6387 Val Acc: 0.6948\n",
      "Epoch [893/1000] Train Loss: 0.4787 Val Loss: 0.6391 Val Acc: 0.6948\n",
      "Epoch [894/1000] Train Loss: 0.4835 Val Loss: 0.6393 Val Acc: 0.6948\n",
      "Epoch [895/1000] Train Loss: 0.4767 Val Loss: 0.6390 Val Acc: 0.6948\n",
      "Epoch [896/1000] Train Loss: 0.4851 Val Loss: 0.6387 Val Acc: 0.6988\n",
      "Epoch [897/1000] Train Loss: 0.4971 Val Loss: 0.6394 Val Acc: 0.6948\n",
      "Epoch [898/1000] Train Loss: 0.4867 Val Loss: 0.6388 Val Acc: 0.6948\n",
      "Epoch [899/1000] Train Loss: 0.4833 Val Loss: 0.6379 Val Acc: 0.6988\n",
      "Epoch [900/1000] Train Loss: 0.4902 Val Loss: 0.6374 Val Acc: 0.6988\n",
      "Epoch [901/1000] Train Loss: 0.4685 Val Loss: 0.6377 Val Acc: 0.6988\n",
      "Epoch [902/1000] Train Loss: 0.4897 Val Loss: 0.6385 Val Acc: 0.6988\n",
      "Epoch [903/1000] Train Loss: 0.4912 Val Loss: 0.6397 Val Acc: 0.6988\n",
      "Epoch [904/1000] Train Loss: 0.4767 Val Loss: 0.6408 Val Acc: 0.6988\n",
      "Epoch [905/1000] Train Loss: 0.4831 Val Loss: 0.6413 Val Acc: 0.6948\n",
      "Epoch [906/1000] Train Loss: 0.4937 Val Loss: 0.6406 Val Acc: 0.6988\n",
      "Epoch [907/1000] Train Loss: 0.4765 Val Loss: 0.6399 Val Acc: 0.6988\n",
      "Epoch [908/1000] Train Loss: 0.4919 Val Loss: 0.6398 Val Acc: 0.6988\n",
      "Epoch [909/1000] Train Loss: 0.4740 Val Loss: 0.6402 Val Acc: 0.6988\n",
      "Epoch [910/1000] Train Loss: 0.4803 Val Loss: 0.6408 Val Acc: 0.6988\n",
      "Epoch [911/1000] Train Loss: 0.4872 Val Loss: 0.6416 Val Acc: 0.6988\n",
      "Epoch [912/1000] Train Loss: 0.4930 Val Loss: 0.6420 Val Acc: 0.7028\n",
      "Epoch [913/1000] Train Loss: 0.4925 Val Loss: 0.6411 Val Acc: 0.7028\n",
      "Epoch [914/1000] Train Loss: 0.4710 Val Loss: 0.6408 Val Acc: 0.6988\n",
      "Epoch [915/1000] Train Loss: 0.4771 Val Loss: 0.6409 Val Acc: 0.6988\n",
      "Epoch [916/1000] Train Loss: 0.4819 Val Loss: 0.6409 Val Acc: 0.6988\n",
      "Epoch [917/1000] Train Loss: 0.4709 Val Loss: 0.6419 Val Acc: 0.6988\n",
      "Epoch [918/1000] Train Loss: 0.4789 Val Loss: 0.6424 Val Acc: 0.6988\n",
      "Epoch [919/1000] Train Loss: 0.4895 Val Loss: 0.6424 Val Acc: 0.6988\n",
      "Epoch [920/1000] Train Loss: 0.4753 Val Loss: 0.6430 Val Acc: 0.6948\n",
      "Epoch [921/1000] Train Loss: 0.4764 Val Loss: 0.6432 Val Acc: 0.6988\n",
      "Epoch [922/1000] Train Loss: 0.4787 Val Loss: 0.6435 Val Acc: 0.6988\n",
      "Epoch [923/1000] Train Loss: 0.4854 Val Loss: 0.6434 Val Acc: 0.6988\n",
      "Epoch [924/1000] Train Loss: 0.4714 Val Loss: 0.6430 Val Acc: 0.7028\n",
      "Epoch [925/1000] Train Loss: 0.4818 Val Loss: 0.6430 Val Acc: 0.7028\n",
      "Epoch [926/1000] Train Loss: 0.4776 Val Loss: 0.6435 Val Acc: 0.7028\n",
      "Epoch [927/1000] Train Loss: 0.4778 Val Loss: 0.6438 Val Acc: 0.7028\n",
      "Epoch [928/1000] Train Loss: 0.4768 Val Loss: 0.6437 Val Acc: 0.7028\n",
      "Epoch [929/1000] Train Loss: 0.4747 Val Loss: 0.6436 Val Acc: 0.6988\n",
      "Epoch [930/1000] Train Loss: 0.4804 Val Loss: 0.6436 Val Acc: 0.6988\n",
      "Epoch [931/1000] Train Loss: 0.4725 Val Loss: 0.6428 Val Acc: 0.6988\n",
      "Epoch [932/1000] Train Loss: 0.4786 Val Loss: 0.6423 Val Acc: 0.6988\n",
      "Epoch [933/1000] Train Loss: 0.4765 Val Loss: 0.6426 Val Acc: 0.6988\n",
      "Epoch [934/1000] Train Loss: 0.4879 Val Loss: 0.6427 Val Acc: 0.7028\n",
      "Epoch [935/1000] Train Loss: 0.4710 Val Loss: 0.6418 Val Acc: 0.6988\n",
      "Epoch [936/1000] Train Loss: 0.4916 Val Loss: 0.6427 Val Acc: 0.7028\n",
      "Epoch [937/1000] Train Loss: 0.4728 Val Loss: 0.6423 Val Acc: 0.6988\n",
      "Epoch [938/1000] Train Loss: 0.4786 Val Loss: 0.6422 Val Acc: 0.6988\n",
      "Epoch [939/1000] Train Loss: 0.4738 Val Loss: 0.6432 Val Acc: 0.6988\n",
      "Epoch [940/1000] Train Loss: 0.4759 Val Loss: 0.6436 Val Acc: 0.6988\n",
      "Epoch [941/1000] Train Loss: 0.4561 Val Loss: 0.6445 Val Acc: 0.7028\n",
      "Epoch [942/1000] Train Loss: 0.4804 Val Loss: 0.6446 Val Acc: 0.7028\n",
      "Epoch [943/1000] Train Loss: 0.4871 Val Loss: 0.6454 Val Acc: 0.7028\n",
      "Epoch [944/1000] Train Loss: 0.4787 Val Loss: 0.6449 Val Acc: 0.7028\n",
      "Epoch [945/1000] Train Loss: 0.4799 Val Loss: 0.6448 Val Acc: 0.7028\n",
      "Epoch [946/1000] Train Loss: 0.4830 Val Loss: 0.6442 Val Acc: 0.6948\n",
      "Epoch [947/1000] Train Loss: 0.4711 Val Loss: 0.6448 Val Acc: 0.7028\n",
      "Epoch [948/1000] Train Loss: 0.4645 Val Loss: 0.6453 Val Acc: 0.7028\n",
      "Epoch [949/1000] Train Loss: 0.4732 Val Loss: 0.6462 Val Acc: 0.7028\n",
      "Epoch [950/1000] Train Loss: 0.4737 Val Loss: 0.6460 Val Acc: 0.7028\n",
      "Epoch [951/1000] Train Loss: 0.4823 Val Loss: 0.6460 Val Acc: 0.6988\n",
      "Epoch [952/1000] Train Loss: 0.4659 Val Loss: 0.6457 Val Acc: 0.6988\n",
      "Epoch [953/1000] Train Loss: 0.4681 Val Loss: 0.6456 Val Acc: 0.6988\n",
      "Epoch [954/1000] Train Loss: 0.4776 Val Loss: 0.6458 Val Acc: 0.7028\n",
      "Epoch [955/1000] Train Loss: 0.4731 Val Loss: 0.6460 Val Acc: 0.7028\n",
      "Epoch [956/1000] Train Loss: 0.4785 Val Loss: 0.6460 Val Acc: 0.6988\n",
      "Epoch [957/1000] Train Loss: 0.4533 Val Loss: 0.6474 Val Acc: 0.7028\n",
      "Epoch [958/1000] Train Loss: 0.4704 Val Loss: 0.6472 Val Acc: 0.6988\n",
      "Epoch [959/1000] Train Loss: 0.4792 Val Loss: 0.6468 Val Acc: 0.6988\n",
      "Epoch [960/1000] Train Loss: 0.4748 Val Loss: 0.6471 Val Acc: 0.6988\n",
      "Epoch [961/1000] Train Loss: 0.4710 Val Loss: 0.6469 Val Acc: 0.6988\n",
      "Epoch [962/1000] Train Loss: 0.4741 Val Loss: 0.6472 Val Acc: 0.6988\n",
      "Epoch [963/1000] Train Loss: 0.4724 Val Loss: 0.6475 Val Acc: 0.6988\n",
      "Epoch [964/1000] Train Loss: 0.4832 Val Loss: 0.6485 Val Acc: 0.6988\n",
      "Epoch [965/1000] Train Loss: 0.4629 Val Loss: 0.6492 Val Acc: 0.6988\n",
      "Epoch [966/1000] Train Loss: 0.4626 Val Loss: 0.6486 Val Acc: 0.6988\n",
      "Epoch [967/1000] Train Loss: 0.4697 Val Loss: 0.6488 Val Acc: 0.6988\n",
      "Epoch [968/1000] Train Loss: 0.4624 Val Loss: 0.6496 Val Acc: 0.6948\n",
      "Epoch [969/1000] Train Loss: 0.4704 Val Loss: 0.6501 Val Acc: 0.6988\n",
      "Epoch [970/1000] Train Loss: 0.4692 Val Loss: 0.6498 Val Acc: 0.6988\n",
      "Epoch [971/1000] Train Loss: 0.4522 Val Loss: 0.6498 Val Acc: 0.6988\n",
      "Epoch [972/1000] Train Loss: 0.4783 Val Loss: 0.6493 Val Acc: 0.6988\n",
      "Epoch [973/1000] Train Loss: 0.4713 Val Loss: 0.6488 Val Acc: 0.6988\n",
      "Epoch [974/1000] Train Loss: 0.4729 Val Loss: 0.6492 Val Acc: 0.6988\n",
      "Epoch [975/1000] Train Loss: 0.4698 Val Loss: 0.6498 Val Acc: 0.6988\n",
      "Epoch [976/1000] Train Loss: 0.4744 Val Loss: 0.6503 Val Acc: 0.6988\n",
      "Epoch [977/1000] Train Loss: 0.4665 Val Loss: 0.6509 Val Acc: 0.6988\n",
      "Epoch [978/1000] Train Loss: 0.4602 Val Loss: 0.6516 Val Acc: 0.6988\n",
      "Epoch [979/1000] Train Loss: 0.4617 Val Loss: 0.6512 Val Acc: 0.6988\n",
      "Epoch [980/1000] Train Loss: 0.4701 Val Loss: 0.6506 Val Acc: 0.6988\n",
      "Epoch [981/1000] Train Loss: 0.4690 Val Loss: 0.6500 Val Acc: 0.6988\n",
      "Epoch [982/1000] Train Loss: 0.4598 Val Loss: 0.6497 Val Acc: 0.6988\n",
      "Epoch [983/1000] Train Loss: 0.4578 Val Loss: 0.6495 Val Acc: 0.6988\n",
      "Epoch [984/1000] Train Loss: 0.4699 Val Loss: 0.6502 Val Acc: 0.6988\n",
      "Epoch [985/1000] Train Loss: 0.4658 Val Loss: 0.6506 Val Acc: 0.6988\n",
      "Epoch [986/1000] Train Loss: 0.4660 Val Loss: 0.6511 Val Acc: 0.6908\n",
      "Epoch [987/1000] Train Loss: 0.4676 Val Loss: 0.6506 Val Acc: 0.6988\n",
      "Epoch [988/1000] Train Loss: 0.4710 Val Loss: 0.6494 Val Acc: 0.7028\n",
      "Epoch [989/1000] Train Loss: 0.4705 Val Loss: 0.6494 Val Acc: 0.6988\n",
      "Epoch [990/1000] Train Loss: 0.4561 Val Loss: 0.6497 Val Acc: 0.7028\n",
      "Epoch [991/1000] Train Loss: 0.4637 Val Loss: 0.6502 Val Acc: 0.7028\n",
      "Epoch [992/1000] Train Loss: 0.4753 Val Loss: 0.6500 Val Acc: 0.7028\n",
      "Epoch [993/1000] Train Loss: 0.4714 Val Loss: 0.6496 Val Acc: 0.7028\n",
      "Epoch [994/1000] Train Loss: 0.4521 Val Loss: 0.6497 Val Acc: 0.6988\n",
      "Epoch [995/1000] Train Loss: 0.4579 Val Loss: 0.6514 Val Acc: 0.6948\n",
      "Epoch [996/1000] Train Loss: 0.4617 Val Loss: 0.6518 Val Acc: 0.6908\n",
      "Epoch [997/1000] Train Loss: 0.4697 Val Loss: 0.6524 Val Acc: 0.6908\n",
      "Epoch [998/1000] Train Loss: 0.4483 Val Loss: 0.6527 Val Acc: 0.6908\n",
      "Epoch [999/1000] Train Loss: 0.4590 Val Loss: 0.6531 Val Acc: 0.6948\n",
      "Epoch [1000/1000] Train Loss: 0.4567 Val Loss: 0.6526 Val Acc: 0.6988\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = MLP(input_dim)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "num_epochs = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * batch_X.size(0)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    \n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item() * batch_X.size(0)\n",
    "            \n",
    "            predicted = (outputs >= 0.5).float()\n",
    "            total += batch_y.size(0)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f} \"\n",
    "          f\"Val Loss: {val_loss:.4f} \"\n",
    "          f\"Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # domain knowledge from literature\n",
    "# critical_pairs = {\n",
    "#     'schizophrenia': ['Fz-Pz', 'F3-F4'],\n",
    "#     'mood_disorder': ['F3-P3', 'F4-P4'],\n",
    "#     'addictive_disorder': ['F7-F8']\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
